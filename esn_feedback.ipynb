{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aW_6ADldkbuB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW_6ADldkbuB",
        "outputId": "fe8306c7-3a8e-4165-e62e-41a424a840b9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_QJywZpKBcrP",
      "metadata": {
        "id": "_QJywZpKBcrP"
      },
      "outputs": [],
      "source": [
        "directory='.'\n",
        "# directory='/content/drive/MyDrive/esn2sparse'; \n",
        "# !cp \"/content/drive/MyDrive/esn2sparse/params.py\" \".\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "0caa84e4",
      "metadata": {
        "id": "0caa84e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import sympy as sp\n",
        "from scipy import stats\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as pl\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import time\n",
        "import torch.jit as jit\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from os.path import exists\n",
        "import gc\n",
        "import importlib\n",
        "import params_feedback as par\n",
        "import time\n",
        "import os\n",
        "# device = 'cpu'\n",
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "e75f2936",
      "metadata": {
        "id": "e75f2936"
      },
      "outputs": [],
      "source": [
        "mnist_trainset = datasets.MNIST(root=directory+'/data', train=True, download=True, transform=None)\n",
        "mnist_testset = datasets.MNIST(root=directory+'/data', train=False, download=True, transform=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "d9c42c75",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9c42c75",
        "outputId": "0928ebb6-40ac-4b31-b171-06853d2d74fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_tr shape = torch.Size([50000, 28, 28]),    Y_tr shape = torch.Size([50000, 10])\n"
          ]
        }
      ],
      "source": [
        "X_te=mnist_testset.data               ## Test set images\n",
        "y_te=mnist_testset.test_labels        ## Test set labels\n",
        "\n",
        "N_o=10                                ## Number of output nodes/classes\n",
        "N_te=y_te.size()[0]                   ## Number of test samples\n",
        "Y_te=torch.zeros([N_te,N_o])          ## Initialisation of the one-hot encoded labels for the test set\n",
        "Y_te[np.arange(0,N_te),y_te]=1        ## From labels to one-hot encoded labels for the test set\n",
        "\n",
        "X_tr=mnist_trainset.data              ## Train set images\n",
        "y_tr=mnist_trainset.train_labels      ## Train labels \n",
        "N_tr=y_tr.size()[0]                   ## Number of training samples\n",
        "N_i = X_tr.size()[1]                  ## Number of inputs to ESN\n",
        "\n",
        "Y_tr=torch.zeros([N_tr,N_o])          ## Initialisation of one-hot encoded labels for training\n",
        "Y_tr[np.arange(0,N_tr),y_tr]=1        ## From labels to one-hot encoded labels for the training set\n",
        "\n",
        "N_val=10000                           ## Here I take out N_val samples from the training set and use them for validation\n",
        "i_val=np.random.permutation(np.arange(0,N_tr))[0:N_val]\n",
        "\n",
        "X_val=X_tr[i_val,:,:]\n",
        "Y_val=Y_tr[i_val,:]\n",
        "\n",
        "i_tr=np.delete(np.arange(0,N_tr),i_val)\n",
        "N_tr=N_tr-N_val\n",
        "\n",
        "X_tr=X_tr[i_tr,:,:]\n",
        "Y_tr=Y_tr[i_tr,:]\n",
        "\n",
        "T=X_tr.size()[2]\n",
        "N_in=X_tr.size()[1]\n",
        "\n",
        "## Normalisation and conversion to float\n",
        "X_M=255\n",
        "# X_tr=torch.reshape( (X_tr.float()/X_M),[-1,784]) \n",
        "# X_val=torch.reshape((X_val.float()/X_M),[-1,784])\n",
        "# X_te=torch.reshape((X_te.float()/X_M),[-1,784])\n",
        "X_tr=torch.reshape( (X_tr.float()),[-1,784]).to(device)\n",
        "X_val=torch.reshape((X_val.float()),[-1,784]).to(device)\n",
        "X_te=torch.reshape((X_te.float()),[-1,784]).to(device)\n",
        "\n",
        "for j in range(X_tr.shape[0]):\n",
        "    X_tr[j,:] -= torch.mean(X_tr[j,:])\n",
        "    X_tr[j,:] /= torch.std(X_tr[j,:])\n",
        "\n",
        "for j in range(X_val.shape[0]):\n",
        "    X_val[j,:] -= torch.mean(X_val[j,:])\n",
        "    X_val[j,:] /= torch.std(X_val[j,:])\n",
        "\n",
        "for j in range(X_te.shape[0]):\n",
        "    X_te[j,:] -= torch.mean(X_te[j,:])\n",
        "    X_te[j,:] /= torch.std(X_te[j,:])\n",
        "\n",
        "X_tr=torch.reshape( (X_tr),[-1,28,28]) \n",
        "X_val=torch.reshape((X_val),[-1,28,28])\n",
        "X_te=torch.reshape((X_te),[-1,28,28])\n",
        "\n",
        "Y_tr=Y_tr.float().to(device)\n",
        "Y_val=Y_val.float().to(device)\n",
        "Y_te=Y_te.float().to(device)\n",
        "\n",
        "print(f'X_tr shape = {X_tr.shape},    Y_tr shape = {Y_tr.shape}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2148197e",
      "metadata": {
        "id": "2148197e"
      },
      "outputs": [],
      "source": [
        "# Add random contrast transformations to input data (OLD VERSION - results in outlines)\n",
        "for j in range(X_tr.shape[0]):\n",
        "    X_tr[j,X_tr[j,:]==X_tr[j,:].min()] = X_tr[j,X_tr[j,:]==X_tr[j,:].min()] + torch.rand(1).to(device) * torch.tensor(2.).to(device) * (X_tr[j,:].max() - X_tr[j,:].min())\n",
        "for j in range(X_val.shape[0]):\n",
        "    X_val[j,X_val[j,:]==X_val[j,:].min()] = X_val[j,X_val[j,:]==X_val[j,:].min()] + torch.rand(1).to(device) * torch.tensor(2.).to(device) * (X_val[j,:].max() - X_val[j,:].min())\n",
        "for j in range(X_te.shape[0]):\n",
        "    X_te[j,X_te[j,:]==X_te[j,:].min()] = X_te[j,X_te[j,:]==X_te[j,:].min()] + torch.rand(1).to(device) * torch.tensor(2.).to(device) * (X_te[j,:].max() - X_te[j,:].min())\n",
        "\n",
        "# Z-score inputs\n",
        "for j in range(X_tr.shape[0]):\n",
        "    X_tr[j,:] -= torch.mean(X_tr[j,:])\n",
        "    X_tr[j,:] /= torch.std(X_tr[j,:])\n",
        "\n",
        "for j in range(X_val.shape[0]):\n",
        "    X_val[j,:] -= torch.mean(X_val[j,:])\n",
        "    X_val[j,:] /= torch.std(X_val[j,:])\n",
        "\n",
        "for j in range(X_te.shape[0]):\n",
        "    X_te[j,:] -= torch.mean(X_te[j,:])\n",
        "    X_te[j,:] /= torch.std(X_te[j,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8a64e52",
      "metadata": {
        "id": "d8a64e52"
      },
      "outputs": [],
      "source": [
        "# Add random contrast transformations to input data\n",
        "contrast_mean = torch.tensor(0.5).to(device)\n",
        "contrast_range = torch.tensor(0.99).to(device)\n",
        "# Training data\n",
        "for j in range(X_tr.shape[0]):\n",
        "    # Rescale values to range 0-1\n",
        "    X_tr[j,:] -= torch.min(X_tr[j,:])\n",
        "    X_tr[j,:] /= torch.max(X_tr[j,:])\n",
        "    # Reverse contrast with prob. = 0.5\n",
        "    if torch.rand(1)>0.5:\n",
        "        X_tr[j,:] = torch.tensor(1.0).to(device) - X_tr[j,:]\n",
        "    # Z-score\n",
        "    X_tr[j,:] -= torch.mean(X_tr[j,:])\n",
        "    X_tr[j,:] /= (torch.std(X_tr[j,:]) / (contrast_mean + contrast_range*(torch.rand(1).to(device) - torch.tensor(0.5).to(device))))\n",
        "\n",
        "# Validation data\n",
        "for j in range(X_val.shape[0]):\n",
        "    # Rescale values to range 0-1\n",
        "    X_val[j,:] -= torch.min(X_val[j,:])\n",
        "    X_val[j,:] /= torch.max(X_val[j,:])\n",
        "    # Reverse contrast with prob. = 0.5\n",
        "    if torch.rand(1)>0.5:\n",
        "        X_val[j,:] = torch.tensor(1.0).to(device) - X_val[j,:]\n",
        "    # Z-score\n",
        "    X_val[j,:] -= torch.mean(X_val[j,:])\n",
        "    X_val[j,:] /= (torch.std(X_val[j,:]) / (contrast_mean + contrast_range*(torch.rand(1).to(device) - torch.tensor(0.5).to(device))))\n",
        "\n",
        "# Test data\n",
        "for j in range(X_te.shape[0]):\n",
        "    # Rescale values to range 0-1\n",
        "    X_te[j,:] -= torch.min(X_te[j,:])\n",
        "    X_te[j,:] /= torch.max(X_te[j,:])\n",
        "    # Reverse contrast with prob. = 0.5\n",
        "    if torch.rand(1)>0.5:\n",
        "        X_te[j,:] = torch.tensor(1.0).to(device) - X_te[j,:]\n",
        "    # Z-score\n",
        "    X_te[j,:] -= torch.mean(X_te[j,:])\n",
        "    X_te[j,:] /= (torch.std(X_te[j,:]) / (contrast_mean + contrast_range*(torch.rand(1).to(device) - torch.tensor(0.5).to(device))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04a05756",
      "metadata": {
        "id": "04a05756"
      },
      "outputs": [],
      "source": [
        "# IF NOT USING ESN\n",
        "# Make inputs 2800-dim and z-score\n",
        "from scipy import stats\n",
        "with torch.no_grad():\n",
        "    w = torch.randn([784, par.N_esn*28]).to(device)/torch.sqrt(torch.tensor(par.N_esn*28+784)).to(device)\n",
        "    batchsize = torch.tensor(100).to(device)\n",
        "    # Operate on batches of data to save GPU-memory\n",
        "    # Training data\n",
        "    nbatch = torch.ceil(X_tr.shape[0]/batchsize).int()\n",
        "    temp = torch.zeros(batchsize, par.N_esn*28).to(device)\n",
        "    perm = torch.zeros(X_tr.shape[0], par.N_esn*28).cpu()\n",
        "    for j in range(nbatch):    \n",
        "        temp = torch.matmul(X_tr[j*batchsize:min((j+1)*batchsize,X_tr.shape[0]),:,:].reshape(batchsize, X_tr.shape[1]*X_tr.shape[2]), w)\n",
        "        temp -= torch.matmul(temp.mean(dim=1, keepdim=True), torch.ones(1, par.N_esn * 28).to(device))\n",
        "        temp /= temp.std()\n",
        "        perm[j*batchsize:min((j+1)*batchsize,X_tr.shape[0]), :] = torch.clone(temp).to(device)\n",
        "    X_tr = torch.clone(perm).cpu()\n",
        "    # Validation data\n",
        "    nbatch = torch.ceil(X_val.shape[0]/batchsize).int()\n",
        "    temp = torch.zeros(batchsize, par.N_esn*28).to(device)\n",
        "    perm = torch.zeros(X_val.shape[0], par.N_esn*28).cpu()\n",
        "    for j in range(nbatch):    \n",
        "        temp = torch.matmul(X_val[j*batchsize:min((j+1)*batchsize,X_val.shape[0]),:,:].reshape(batchsize, X_val.shape[1]*X_val.shape[2]), w)\n",
        "        temp -= torch.matmul(temp.mean(dim=1, keepdim=True), torch.ones(1, par.N_esn * 28).to(device))\n",
        "        temp /= temp.std()\n",
        "        perm[j*batchsize:min((j+1)*batchsize,X_val.shape[0]), :] = torch.clone(temp).to(device)\n",
        "    X_val = torch.clone(perm).cpu()\n",
        "    # Test data\n",
        "    nbatch = torch.ceil(X_te.shape[0]/batchsize).int()\n",
        "    temp = torch.zeros(batchsize, par.N_esn*28).to(device)\n",
        "    perm = torch.zeros(X_te.shape[0], par.N_esn*28).cpu()\n",
        "    for j in range(nbatch):    \n",
        "        temp = torch.matmul(X_te[j*batchsize:min((j+1)*batchsize,X_te.shape[0]),:,:].reshape(batchsize, X_te.shape[1]*X_te.shape[2]), w)\n",
        "        temp -= torch.matmul(temp.mean(dim=1, keepdim=True), torch.ones(1, par.N_esn * 28).to(device))\n",
        "        temp /= temp.std()\n",
        "        perm[j*batchsize:min((j+1)*batchsize,X_te.shape[0]), :] = torch.clone(temp).to(device)\n",
        "    X_te = torch.clone(perm).cpu()\n",
        "    \n",
        "    # X_tr -= torch.matmul(X_tr.mean(dim=1, keepdim=True), torch.ones(1, par.N_esn * 28).to(device))\n",
        "    # X_tr /= X_tr.mean()\n",
        "    # X_tr -= torch.matmul(X_tr.mean(dim=1, keepdim=True), torch.ones(1, par.N_esn * 28).to(device))\n",
        "    # X_tr /= X_tr.mean()\n",
        "\n",
        "    # X_tr = torch.tensor(stats.zscore(torch.matmul(X_tr.reshape(X_tr.shape[0], X_tr.shape[1]*X_tr.shape[2]), w).numpy(), axis=1))\n",
        "    # X_val = torch.tensor(stats.zscore(torch.matmul(X_val.reshape(X_val.shape[0], X_val.shape[1]*X_val.shape[2]), w).numpy(), axis=1))\n",
        "    # X_te = torch.tensor(stats.zscore(torch.matmul(X_te.reshape(X_te.shape[0], X_te.shape[1]*X_te.shape[2]), w).numpy(), axis=1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "3f55bc9b",
      "metadata": {
        "id": "3f55bc9b"
      },
      "outputs": [],
      "source": [
        "def Data2Classes(X,Y):\n",
        "    \n",
        "    ind=torch.where(Y==1)[1]\n",
        "\n",
        "    N_class=torch.max(ind)+1\n",
        "    \n",
        "    X1=[]\n",
        "    Y1=[]\n",
        "    \n",
        "    for n in range(N_class):\n",
        "    \n",
        "        ind1=torch.where(ind==n)[0].type(torch.long)\n",
        "\n",
        "        X1.append(X[ind1,:].to(device))\n",
        "        Y1.append(Y[ind1,:].to(device))\n",
        "        \n",
        "    return X1, Y1\n",
        "        \n",
        "# X_tr/X_val/X_te are lists of length 10 (1 entry per class)\n",
        "X_tr, Y_tr=Data2Classes(X_tr,Y_tr)\n",
        "\n",
        "X_val, Y_val=Data2Classes(X_val,Y_val)\n",
        "\n",
        "X_te, Y_te=Data2Classes(X_te,Y_te)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zhLHzeGYPsE1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "zhLHzeGYPsE1",
        "outputId": "c15a1dbb-6850-4f41-febc-a380d4d35bae"
      },
      "outputs": [],
      "source": [
        "# Plot activity and correlation between classes\n",
        "N_esn = par.N_esn\n",
        "a = np.zeros((1000,N_esn*28))\n",
        "for j in range(len(X_tr)):\n",
        "    a[j*100:(j+1)*100,:] = X_tr[j][0:100,:].numpy()\n",
        "print(np.max(a), np.min(a))\n",
        "c = np.matmul(stats.zscore(a,axis=1), np.transpose(stats.zscore(a,axis=1))) / a.shape[1]\n",
        "fig = pl.figure(figsize=tuple(np.array((50.,20.))/2.54)); ax = pl.axes()\n",
        "imdata = ax.imshow(X_tr[0][0:1000,:],vmin=-2.0, vmax=2.0)\n",
        "fig = pl.figure(figsize=tuple(np.array((50.,20.))/2.54)); ax = pl.axes()\n",
        "imdata = ax.imshow(X_tr[2][0:1000,:],vmin=-2.0, vmax=2.0)\n",
        "fig = pl.figure(figsize=tuple(np.array((5.,5.))/2.54)); ax = pl.axes()\n",
        "imdata = ax.imshow(c,vmin=-1.0, vmax=1.0)\n",
        "# cb = fig.colorbar(imdata, ticks=[0, 1.0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b7cbfc3",
      "metadata": {
        "id": "8b7cbfc3"
      },
      "outputs": [],
      "source": [
        "class NN(nn.Module):\n",
        "    \n",
        "    def __init__(self,Ns,N_class):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.N_class=N_class\n",
        "        N_layers=np.shape(Ns)[0]\n",
        "        \n",
        "        self.Ws=[]\n",
        "        self.bs=[]\n",
        "        \n",
        "        self.Ths=[]\n",
        "        self.Th_bs=[]\n",
        "        \n",
        "        self.Ns=Ns\n",
        "        for n in range(1,np.shape(Ns)[0]):\n",
        "        \n",
        "            self.Ws.append(nn.Parameter(torch.randn([Ns[n-1],Ns[n]]).to(device)/torch.sqrt(torch.tensor(Ns[n-1]+Ns[n]))))\n",
        "            self.bs.append(nn.Parameter(torch.zeros([Ns[n]]).to(device)))\n",
        "\n",
        "        \n",
        "        self.Ths.append( nn.Parameter( torch.randn([Ns[-1],N_class])/torch.sqrt(torch.tensor(Ns[n-1]+Ns[n])) ) ) \n",
        "        self.Th_bs.append( nn.Parameter(torch.zeros([N_class])) )\n",
        "        \n",
        "    def Initialise_Hyperparameters(self,eta_t, eta_o,batch_size,margin):\n",
        "\n",
        "\n",
        "        self.eta_t=eta_t\n",
        "        self.eta_o=eta_o\n",
        "        self.batch_size=batch_size\n",
        "\n",
        "        self.opt=optim.Adam([{ 'params': self.Ws+self.bs, 'lr':eta_t }])\n",
        "        \n",
        "        self.opt_out=optim.Adam([{ 'params': self.Ths+self.Th_bs, 'lr':eta_o }])\n",
        "                              \n",
        "        self.margin=margin\n",
        "        \n",
        "        \n",
        "    def Forward_Triplet(self, S):\n",
        "        \n",
        "        ys=[]\n",
        "        ns = S.shape[1] # Number of samples\n",
        "        ACC = 0.0\n",
        "        o = torch.ones(9,1)\n",
        "        for k in range(3):\n",
        "        \n",
        "            xs=[]\n",
        "            xs.append(S[k,:,:])\n",
        "\n",
        "            for n in range(0,np.shape(self.Ns)[0]-2):\n",
        "                # Compute activities in each layer\n",
        "                xs.append(torch.relu( torch.add(torch.matmul(xs[n],self.Ws[n]),self.bs[n]) ) )\n",
        "\n",
        "            ys.append( torch.add(torch.matmul(xs[-1],self.Ws[-1]),self.bs[-1]) )\n",
        "        \n",
        "        # # Use Euclidean distance\n",
        "        # dneg = torch.sum( torch.pow( ys[0].detach()-ys[2].detach(), 2 ), 1)\n",
        "        # dposneg = torch.sum( torch.pow( ys[1].detach()-ys[2].detach(), 2 ), 1)\n",
        "        # # Find the hard negative pairs and swap anchor and positive if dposneg>dneg\n",
        "        # ind = dneg > dposneg\n",
        "        # acopy = torch.clone(ys[0])\n",
        "        # ys[0][ind,:] = torch.clone(ys[1][ind,:])\n",
        "        # ys[1][ind,:] = torch.clone(acopy[ind,:])\n",
        "        # Recompute distances\n",
        "        dpos = torch.sum( torch.pow( ys[0]-ys[1], 2 ), 1)\n",
        "        dneg = torch.sum( torch.pow( ys[0]-ys[2], 2 ), 1)\n",
        "        \n",
        "        E = dpos - dneg + self.margin\n",
        "        E = torch.mean( E*(E>0) )\n",
        "        if E>0:\n",
        "            \n",
        "            E.backward()\n",
        "        \n",
        "            self.opt.step()\n",
        "            self.opt.zero_grad()\n",
        "        \n",
        "        # # Use correlation\n",
        "        # a = (ys[0] - torch.matmul(torch.mean(ys[0],1,keepdim=True), torch.ones(1,ys[0].shape[1]))) / torch.matmul(torch.std(ys[0],1,keepdim=True), torch.ones(1,ys[0].shape[1]))\n",
        "        # p = (ys[1] - torch.matmul(torch.mean(ys[1],1,keepdim=True), torch.ones(1,ys[0].shape[1]))) / torch.matmul(torch.std(ys[1],1,keepdim=True), torch.ones(1,ys[0].shape[1]))\n",
        "        # n = (ys[2] - torch.matmul(torch.mean(ys[2],1,keepdim=True), torch.ones(1,ys[0].shape[1]))) / torch.matmul(torch.std(ys[2],1,keepdim=True), torch.ones(1,ys[0].shape[1]))\n",
        "        # dpos = torch.sum( torch.mul(a,p) ,1) / a.shape[1]\n",
        "        # dneg = torch.sum( torch.mul(a,n) ,1) / a.shape[1]\n",
        "        # E = torch.mean(dneg - dpos)\n",
        "        \n",
        "        # E.backward()\n",
        "    \n",
        "        # self.opt.step()\n",
        "        # self.opt.zero_grad()\n",
        "\n",
        "        # print(f'-----  Triplet loss = {torch.mean(E)},-----  +ve = {torch.mean(dpos)},------  -ve = {torch.mean(dneg)}')\n",
        "        \n",
        "        dn = torch.zeros(ns)\n",
        "        for j in range(ns):            \n",
        "            ind1 = torch.randperm(ns)\n",
        "            dn = torch.sum(torch.pow(torch.matmul(o, ys[0][j,None,:]) - ys[2][ind1[0:9],:], 2.), 1)\n",
        "            ACC += torch.any(dpos[j]*o > dn).logical_not().long()\n",
        "        ACC /= ns\n",
        "\n",
        "        return E, ACC, torch.mean(dpos), torch.mean(dneg)\n",
        "        \n",
        "        \n",
        "    def Forward_out(self, S, Y):\n",
        "       \n",
        "        \n",
        "        xs=[]\n",
        "        xs.append(S)\n",
        "\n",
        "        for n in range(0,np.shape(self.Ns)[0]-2): # For each layer except the final layer\n",
        "            # Build up training data so that outputs learn from all previous layers\n",
        "            xs.append(torch.relu( torch.add(torch.matmul(xs[n],self.Ws[n]),self.bs[n]) ) )\n",
        "\n",
        "        y= torch.add(torch.matmul(xs[-1],self.Ws[-1]),self.bs[-1])\n",
        "        out=torch.add(torch.matmul(y.detach(),self.Ths[0]),self.Th_bs[0]) \n",
        "        \n",
        "        E_out=torch.mean(torch.sum( torch.pow( out-Y, 2 ), 1))\n",
        "        \n",
        "        Acc=torch.mean(torch.eq( torch.argmax(out,1), torch.argmax(Y,1) ).type(torch.float))\n",
        "        \n",
        "        E_out.backward()\n",
        "        \n",
        "        self.opt_out.step()\n",
        "        self.opt_out.zero_grad()\n",
        "        \n",
        "        \n",
        "        return E_out, Acc\n",
        "        \n",
        "        \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H-p_rD-i5OWR",
      "metadata": {
        "id": "H-p_rD-i5OWR"
      },
      "outputs": [],
      "source": [
        "class MLPclassic(nn.Module):\n",
        "    \n",
        "    def __init__(self,par):\n",
        "        super().__init__()\n",
        "\n",
        "        self.N_class=par.nClass\n",
        "\n",
        "        self.N = par.N_esn\n",
        "        self.alpha = par.alpha\n",
        "        self.rho = par.rho\n",
        "        self.N_av = par.N_av\n",
        "        self.N_i = par.nInputs\n",
        "        self.gamma = par.gamma\n",
        "\n",
        "        dilution = 1-self.N_av/self.N\n",
        "        W = np.random.uniform(-1, 1, [self.N, self.N])\n",
        "        W = W*(np.random.uniform(0, 1, [self.N, self.N]) > dilution)\n",
        "        eig = np.linalg.eigvals(W)\n",
        "        self.W = torch.from_numpy(\n",
        "            self.rho*W/(np.max(np.absolute(eig)))).float()\n",
        "\n",
        "        self.x = []\n",
        "\n",
        "        if self.N_i == 1:\n",
        "\n",
        "            self.W_in = 2*np.random.randint(0, 2, [self.N_i, self.N])-1\n",
        "            self.W_in = torch.from_numpy(self.W_in*self.gamma).float()\n",
        "\n",
        "        else:\n",
        "\n",
        "            self.W_in = np.random.randn(self.N_i, self.N)\n",
        "            self.W_in = torch.from_numpy(self.gamma*self.W_in).float()\n",
        "\n",
        "        self.Ws=[]\n",
        "        self.bs=[]\n",
        "        \n",
        "        self.Ns=par.Ns\n",
        "        for n in range(1,np.shape(self.Ns)[0]):\n",
        "        \n",
        "            self.Ws.append(nn.Parameter(torch.randn([self.Ns[n-1],self.Ns[n]]).to(device)/torch.sqrt(torch.tensor(self.Ns[n-1]+self.Ns[n]))))\n",
        "            self.bs.append(nn.Parameter(torch.zeros([self.Ns[n]]).to(device)))\n",
        "        \n",
        "        if par.fbLayer:\n",
        "            self.fbLayer = par.fbLayer\n",
        "            self.W_fb = nn.Parameter(torch.randn([self.Ns[self.fbLayer],self.N])/10**4).to(device)\n",
        "\n",
        "    def Initialise_Hyperparameters(self,eta,batch_size):\n",
        "\n",
        "        self.eta=eta\n",
        "        self.batch_size=batch_size\n",
        "        if hasattr(self,'fbLayer'):\n",
        "            self.opt=optim.Adam([{ 'params': self.Ws+self.bs+[self.W_fb], 'lr':eta }])\n",
        "        else:\n",
        "            self.opt=optim.Adam([{ 'params': self.Ws+self.bs, 'lr':eta }])\n",
        "        \n",
        "    def Forward(self):\n",
        "\n",
        "        for n in range(0,len(self.Ns)-1): # For each layer\n",
        "            # Build up training data so that outputs learn from all previous layers\n",
        "            self.x[n+1] = torch.clone(torch.relu( torch.add(torch.matmul(self.x[n],self.Ws[n]),self.bs[n]) ))\n",
        "\n",
        "    def Reset(self, nSamples):\n",
        "\n",
        "        self.x = []\n",
        "        for n in range(0,len(self.Ns)): # For each layer\n",
        "            self.x.append(torch.zeros(nSamples, self.Ns[n], requires_grad=True))\n",
        "\n",
        "    def ESN_1step(self, s, t):\n",
        "        if t>0:\n",
        "            if hasattr(self,'fbLayer'):\n",
        "                self.x[0] = torch.clone((1-self.alpha)*self.x[0]+self.alpha * \\\n",
        "                    torch.tanh(torch.matmul(s, self.W_in)+torch.matmul(self.x[0], self.W)+torch.matmul(self.x[self.fbLayer], self.W_fb)))\n",
        "            else:\n",
        "                self.x[0] = torch.clone((1-self.alpha)*self.x[0]+self.alpha * \\\n",
        "                    torch.tanh(torch.matmul(s, self.W_in)+torch.matmul(self.x[0], self.W)))\n",
        "        else:\n",
        "            self.x[0] = torch.clone(self.alpha * torch.tanh(torch.matmul(s, self.W_in)))\n",
        "\n",
        "    def response(self, Input, Y, backwardFlag=True):\n",
        "\n",
        "\n",
        "        self.opt.zero_grad()\n",
        "        N_samples = Input.shape[0]\n",
        "        T = Input.shape[2]\n",
        "        E = 0.\n",
        "\n",
        "        self.Reset(N_samples,T)\n",
        "        # YY = torch.tile(Y.unsqueeze(2),[1,1,T])\n",
        "        for t in range(T):\n",
        "\n",
        "            self.ESN_1step(Input[:, :, t], t)\n",
        "            self.Forward()\n",
        "            # E += torch.mean(torch.sum( torch.pow( self.x[-1]-Y, 2 ), 1))\n",
        "    \n",
        "        Acc=torch.mean(torch.eq( torch.argmax(self.x[-1],1), torch.argmax(Y,1) ).type(torch.float))\n",
        "        # Compute Loss\n",
        "        p = torch.div( torch.exp(self.x[-1]), torch.sum(torch.exp(self.x[-1]), 1, keepdim=True).tile((1,self.x[-1].shape[1])) )\n",
        "        ind = range(N_samples)\n",
        "        E = torch.mean(- torch.log(p[ind,torch.argmax(Y,1)]))\n",
        "        # E = torch.mean(torch.sum( torch.pow( self.x[-1]-Y, 2 ), 1))\n",
        "        # E /= T\n",
        "        if backwardFlag:\n",
        "            E.backward()\n",
        "            self.opt.step()\n",
        "\n",
        "        return E,Acc\n",
        "    \n",
        "    def responseSave(self, Input,saveLayers=[]):\n",
        "\n",
        "        N_samples = Input.shape[0]\n",
        "        T = Input.shape[2]\n",
        "        \n",
        "        sav = []\n",
        "        for l in saveLayers:\n",
        "            sav.append(torch.zeros(N_samples, self.Ns[l], T))\n",
        "\n",
        "        self.Reset(N_samples,T)\n",
        "        for t in range(T):\n",
        "\n",
        "            self.ESN_1step(Input[:, :, t], t)\n",
        "            self.Forward(t)\n",
        "            \n",
        "            for li, l in enumerate(saveLayers):\n",
        "                sav[li][:,:,t] = torch.clone(self.x[l].detach())\n",
        "    \n",
        "        return sav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "a4f2ef53",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLPmetric(nn.Module):\n",
        "    \n",
        "    def __init__(self,par):\n",
        "        super().__init__()\n",
        "\n",
        "        self.N_class=par.nClass\n",
        "\n",
        "        self.N = par.N_esn\n",
        "        self.alpha = par.alpha\n",
        "        self.rho = par.rho\n",
        "        self.N_av = par.N_av\n",
        "        self.N_i = par.nInputs\n",
        "        self.gamma = par.gamma\n",
        "        self.tMax = par.tMax\n",
        "        self.batch_size = par.batch_size\n",
        "        self.nSampPerClassPerBatch = int(par.batch_size/par.nClass) # No. input samples per class, per batch\n",
        "        self.maxLayer = par.maxLayer\n",
        "        self.lossLayer = par.lossLayer\n",
        "        self.metricLossType = par.metricLossType\n",
        "        self.margin = par.margin\n",
        "        self.tri = 1 if par.metricLossType=='tripletLoss' else 0\n",
        "        self.wPerf = torch.exp(-torch.arange(self.tMax).flip(0)/par.tauPerf)\n",
        "\n",
        "        dilution = 1-self.N_av/self.N\n",
        "        W = np.random.uniform(-1, 1, [self.N, self.N])\n",
        "        W = W*(np.random.uniform(0, 1, [self.N, self.N]) > dilution)\n",
        "        eig = np.linalg.eigvals(W)\n",
        "        self.W = torch.from_numpy(\n",
        "            self.rho*W/(np.max(np.absolute(eig)))).float().to(device)\n",
        "\n",
        "        self.x = []\n",
        "\n",
        "        if self.N_i == 1:\n",
        "\n",
        "            self.W_in = 2*np.random.randint(0, 2, [self.N_i, self.N])-1\n",
        "            self.W_in = torch.from_numpy(self.W_in*self.gamma).float().to(device)\n",
        "\n",
        "        else:\n",
        "\n",
        "            self.W_in = np.random.randn(self.N_i, self.N)\n",
        "            self.W_in = torch.from_numpy(self.gamma*self.W_in).float().to(device)\n",
        "\n",
        "        self.Ws=[]\n",
        "        self.bs=[]\n",
        "        \n",
        "        self.Ns=par.Ns\n",
        "        self.numW = 0 # Total number of feedforward weights\n",
        "        for n in range(1,np.shape(self.Ns)[0]):\n",
        "        \n",
        "            self.Ws.append(nn.Parameter((torch.randn([self.Ns[n-1],self.Ns[n]])/torch.sqrt(torch.tensor(self.Ns[n-1]+self.Ns[n]))).to(device)))\n",
        "            self.bs.append(nn.Parameter(torch.zeros([self.Ns[n]]).to(device)))\n",
        "\n",
        "            self.numW += float(torch.numel(self.Ws[n-1]))\n",
        "        \n",
        "        if par.fbLayer:\n",
        "            self.fbLayer = par.fbLayer\n",
        "            self.W_fb = nn.Parameter((torch.randn([self.Ns[self.fbLayer],self.N])/10**4).to(device))\n",
        "\n",
        "    def Initialise_Hyperparameters(self,eta,batch_size):\n",
        "\n",
        "        self.eta=eta\n",
        "        self.batch_size=batch_size\n",
        "        if hasattr(self,'fbLayer'):\n",
        "            self.opt=optim.Adam([{ 'params': self.Ws+self.bs+[self.W_fb], 'lr':eta }])\n",
        "        else:\n",
        "            self.opt=optim.Adam([{ 'params': self.Ws+self.bs, 'lr':eta }])\n",
        "        \n",
        "    def Forward(self):\n",
        "\n",
        "        for n in range(0,np.minimum(self.maxLayer+1,len(self.Ns))-1): # For each layer, up to maxLayer\n",
        "            # Build up training data so that outputs learn from all previous layers\n",
        "            # self.x[n+1] = torch.clone(torch.log(torch.tensor(1.) + torch.relu(torch.add(torch.matmul(self.x[n],self.Ws[n]),self.bs[n])) ))\n",
        "            self.x[n+1] = torch.clone(torch.relu( torch.add(torch.matmul(self.x[n],self.Ws[n]),self.bs[n]) ))\n",
        "            # if n==(self.maxLayer-1):\n",
        "            #     # Use Sigmoid for output layer\n",
        "            #     self.x[n+1] = torch.clone(torch.sigmoid( torch.add(torch.matmul(self.x[n],self.Ws[n]),self.bs[n]) ))\n",
        "            # else:\n",
        "            #     # Use ReLU\n",
        "            #     self.x[n+1] = torch.clone(torch.relu( torch.add(torch.matmul(self.x[n],self.Ws[n]),self.bs[n]) ))\n",
        "\n",
        "    def Reset(self, nSamples):\n",
        "\n",
        "        self.x = []\n",
        "        for n in range(0,len(self.Ns)): # For each layer\n",
        "            self.x.append(torch.zeros(nSamples, self.Ns[n], requires_grad=False))\n",
        "\n",
        "    def ESN_1step(self, s, t):\n",
        "        if t>0:\n",
        "            if hasattr(self,'fbLayer'):\n",
        "                self.x[0] = torch.clone((1-self.alpha)*self.x[0]+self.alpha * \\\n",
        "                    torch.tanh(torch.matmul(s, self.W_in)+torch.matmul(self.x[0], self.W)+torch.matmul(self.x[self.fbLayer], self.W_fb)))\n",
        "            else:\n",
        "                self.x[0] = torch.clone((1-self.alpha)*self.x[0]+self.alpha * \\\n",
        "                    torch.tanh(torch.matmul(s, self.W_in)+torch.matmul(self.x[0], self.W)))\n",
        "        else:\n",
        "            self.x[0] = torch.clone(self.alpha * torch.tanh(torch.matmul(s, self.W_in)))\n",
        "\n",
        "    def tripletLoss(self, resp, t):\n",
        "        # # Implement hard negative mining\n",
        "        # W = self.wPerf.expand(resp[0].shape[0],resp[0].shape[1],self.tMax)\n",
        "        # print(f'W has shape {W.shape}')\n",
        "        # print(f'resp[0] has shape {resp[0].shape}')\n",
        "        # print(f'resp[1] has shape {resp[1].shape}')\n",
        "        # print(f'resp[2] has shape {resp[2].shape}')\n",
        "        # self.dAP = ((resp[0] - resp[1]) * W).pow(2).sum(2).sqrt().mean(1) # anchor-positive\n",
        "        # self.dAN = ((resp[0] - resp[2]) * W).pow(2).sum(2).sqrt().mean(1) # anchor-negative\n",
        "        # dPN = ((resp[1] - resp[2]) * W).pow(2).sum(2).sqrt().mean(1) # positive-negative\n",
        "        # Implement hard negative mining\n",
        "\n",
        "        dAP = (resp[0] - resp[1]).pow(2).sum(1).sqrt() # anchor-positive\n",
        "        dAN = (resp[0] - resp[2]).pow(2).sum(1).sqrt() # anchor-negative\n",
        "        \n",
        "        dPN = (resp[1] - resp[2]).pow(2).sum(1).sqrt() # positive-negative\n",
        "        ind = torch.le(dPN, dAN)\n",
        "        ind1 = torch.nonzero(ind)\n",
        "        ind2 = torch.nonzero(torch.logical_not(ind))\n",
        "        # Compute loss\n",
        "        # L = torch.maximum(torch.zeros(resp[0].shape[0]), dAP - dAN + self.margin)\n",
        "        # L = torch.maximum(dAP, dAP - dAN + self.margin)\n",
        "        # if ((np.floor(t/1)) % 2)==0:\n",
        "        #     L = torch.maximum(torch.zeros(resp[0].shape[0]), dAP)\n",
        "        # else:\n",
        "        #     L = torch.maximum(torch.zeros(resp[0].shape[0]), self.margin - dAN)\n",
        "        L = torch.concat((torch.maximum(torch.zeros(ind2.shape).to(device), dAP[ind2] - dAN[ind2] + self.margin), \n",
        "                         torch.maximum(torch.zeros(ind1.shape).to(device), dAP[ind1] - dPN[ind1] + self.margin)), dim=0).mean()\n",
        "        if t%50==0:\n",
        "            print(f'DP = {dAP.mean()}                            DN = {dAN.mean()}                           maxRESP = {resp[0].max()}')\n",
        "\n",
        "        return L.mean()\n",
        "    \n",
        "    def tripletCosineLoss(self, resp):\n",
        "        \n",
        "        # Anchor-positive and anchor-negative cosine distances\n",
        "        dAP = 1. - torch.div(torch.mul(resp[0], resp[1]).sum(1), torch.mul(resp[0].pow(2).sum(1).sqrt(), resp[1].pow(2).sum(1).sqrt())) # anchor-positive\n",
        "        dAN = 1. - torch.div(torch.mul(resp[0], resp[2]).sum(1), torch.mul(resp[0].pow(2).sum(1).sqrt(), resp[2].pow(2).sum(1).sqrt())) # anchor-positive\n",
        "        \n",
        "        # Compute triplet loss\n",
        "        L = torch.maximum(torch.zeros(resp[0].shape[0]), dAP - dAN + 1.0)\n",
        "\n",
        "        return L.mean()\n",
        "    \n",
        "    def tripletAccuracy(self, r):\n",
        "        nSamples = r.shape[0]\n",
        "        # Compute class centroids\n",
        "        centroids = torch.zeros(self.N_class, self.Ns[self.lossLayer]).to(device)\n",
        "        for j in range(self.N_class):\n",
        "            centroids[j,:] = r[j*self.nSampPerClassPerBatch:(j+1)*self.nSampPerClassPerBatch,:].mean(0)\n",
        "        # Compute distances between samples and centroids. Is argmin(dist)==true class?\n",
        "        o = torch.ones(self.N_class,1)\n",
        "        Acc = 0.0\n",
        "        \n",
        "        for j in range(nSamples):     \n",
        "            rr = torch.tile(r[j,:], [self.N_class, 1])\n",
        "            dist = (rr - centroids).pow(2).sum(1)\n",
        "            arg = torch.argmin(dist)       \n",
        "            true_class = torch.floor(torch.tensor(j/self.nSampPerClassPerBatch)).long()\n",
        "            Acc += torch.eq(arg, true_class).float()\n",
        "        Acc /= nSamples\n",
        "        \n",
        "        return Acc\n",
        "    \n",
        "    def tripletCosineAccuracy(self, r):\n",
        "        nSamples = r.shape[0]\n",
        "        # Compute class centroids\n",
        "        centroids = torch.zeros(self.N_class, self.Ns[self.lossLayer])\n",
        "        for j in range(self.N_class):\n",
        "            centroids[j,:] = r[j*self.nSampPerClassPerBatch:(j+1)*self.nSampPerClassPerBatch,:].mean(0)\n",
        "        # Compute distances between samples and centroids. Is argmin(dist)==true class?\n",
        "        o = torch.ones(self.N_class,1)\n",
        "        Acc = 0.0\n",
        "        \n",
        "        for j in range(nSamples):     \n",
        "            rr = torch.tile(r[j,:], [self.N_class, 1])\n",
        "            dist = 1. - torch.div(torch.mul(rr, centroids).sum(1), torch.mul(rr.pow(2).sum(1).sqrt(), centroids.pow(2).sum(1).sqrt()))\n",
        "            arg = torch.argmin(dist)       \n",
        "            true_class = torch.floor(torch.tensor(j/self.nSampPerClassPerBatch)).long()\n",
        "            Acc += torch.eq(arg, true_class).float()\n",
        "        Acc /= nSamples\n",
        "        \n",
        "        return Acc\n",
        "\n",
        "    def response(self, Input, Y, it, backwardFlag=True):\n",
        "\n",
        "        N_samples = int(Input.shape[0] / 3)\n",
        "        T = self.tMax\n",
        "        E = 0.\n",
        "\n",
        "        # Forward pass for anchor, positive, and negative\n",
        "        self.Reset(N_samples * 3)\n",
        "        for t in range(T):\n",
        "            self.ESN_1step(Input[:, :, t], t)\n",
        "            self.Forward()\n",
        "\n",
        "        # Compile responses from layer used to compute loss\n",
        "        r = [] # To store responses from layer used for Loss (create list: [anchor, positive, negative])\n",
        "        for j in range(3):\n",
        "            r.append(torch.clone(self.x[self.lossLayer][j*N_samples:(j+1)*N_samples,:]))                \n",
        "        \n",
        "        # Compute Loss and accuracy\n",
        "        if self.metricLossType=='tripletLoss':\n",
        "            E = self.tripletLoss(r, it)\n",
        "            Acc = self.tripletAccuracy(r[0])\n",
        "        elif self.metricLossType=='tripletCosineLoss':\n",
        "            E = self.tripletCosineLoss(r)\n",
        "            Acc = self.tripletCosineAccuracy(r[0])\n",
        "        elif self.metricLossType=='contrastiveLoss':\n",
        "            E = self.conrastiveLoss(r)\n",
        "            Acc = self.contrastiveAccuracy(r[0])\n",
        "\n",
        "        if backwardFlag:\n",
        "            self.opt.zero_grad()\n",
        "            E.backward()\n",
        "            self.opt.step()\n",
        "            \n",
        "            \n",
        "        return E,Acc\n",
        "    \n",
        "    def responseSave(self, Input,saveLayers=[]):\n",
        "\n",
        "        N_samples = Input.shape[0]\n",
        "        T = self.tMax\n",
        "        \n",
        "        sav = []\n",
        "        for l in saveLayers:\n",
        "            sav.append(torch.zeros(N_samples, self.Ns[l], T))\n",
        "\n",
        "        self.Reset(N_samples)\n",
        "        for t in range(T):\n",
        "            self.ESN_1step(Input[:, :, t], t)\n",
        "            self.Forward()\n",
        "            \n",
        "            for li, l in enumerate(saveLayers):\n",
        "                sav[li][:,:,t] = torch.clone(self.x[l].detach())\n",
        "    \n",
        "        return sav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "GWXIPFSI_Ynd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWXIPFSI_Ynd",
        "outputId": "00ee5616-9405-48e4-b18f-5efaa192ed57"
      },
      "outputs": [],
      "source": [
        "###\n",
        "### WITHOUT metric learning\n",
        "###\n",
        "def xent_esn_fb(expName,rngSeed):\n",
        "    ### Setup directory names\n",
        "    experiment = expName\n",
        "    expDir = directory+'/data/'+experiment\n",
        "    if not os.path.exists(expDir):\n",
        "        os.mkdir(expDir)\n",
        "    outputDir = expDir    # Storage directory for input/label data\n",
        "    if not os.path.exists(outputDir):\n",
        "        os.mkdir(outputDir)\n",
        "\n",
        "    ### Other parameters\n",
        "    save_every = int(np.floor(par.nSaveMaxT / par.nSave)) # Save data every <> epochs, up to epoch nSaveMaxT\n",
        "    tMax = X_tr[0].shape[2]\n",
        "\n",
        "    # Init memory to save responses\n",
        "    if par.saveFlag_RESP:\n",
        "        RESP = []\n",
        "        for layer in par.saveLayers:\n",
        "            RESP.append(np.zeros((par.nSave+1, par.nClass * par.nSaveSamples, par.Ns[layer], tMax))) \n",
        "    # Init memory to save feedback weights\n",
        "    if par.saveFlag_FBWeights:\n",
        "        savWeights = np.zeros((par.Ns[par.fbLayer], par.Ns[0], par.nSave+1))\n",
        "\n",
        "    ### Initialise model\n",
        "    MOD = MLPclassic(par)\n",
        "\n",
        "    MOD.Initialise_Hyperparameters(par.eta,par.batch_size)\n",
        "    L_tr=np.zeros([par.nEpochs])\n",
        "    A_tr=np.zeros([par.nEpochs])\n",
        "    L_val=np.zeros([par.nEpochs])\n",
        "    A_val=np.zeros([par.nEpochs])\n",
        "\n",
        "    print('**********************START TRAINING')\n",
        "    t=time.time()\n",
        "    for n in range(par.nEpochs):\n",
        "        \n",
        "        ### Save met responses before updates\n",
        "        if (n%save_every<1 and n<par.nSaveMaxT) or n==(par.nSaveMaxT-1):\n",
        "            print(f'Saving {int(n/save_every)+1} of {par.nSave+1}')\n",
        "            if par.saveFlag_RESP:\n",
        "                with torch.no_grad():\n",
        "                    s = X_tr[0][0:par.nSaveSamples,:]\n",
        "                    for k in range(1,par.nClass):\n",
        "                        s = torch.concat([s, X_tr[k][0:par.nSaveSamples,:]],0)\n",
        "                    resp = MOD.responseSave(s, par.saveLayers)\n",
        "                    for li, l in enumerate(par.saveLayers):\n",
        "                        RESP[li][int(np.floor(n/save_every)),:,:,:] = resp[li].numpy()\n",
        "            if par.saveFlag_FBWeights:\n",
        "                savWeights[:,:,int(np.floor(n/save_every))] = MOD.W_fb.data\n",
        "\n",
        "        ### Training data\n",
        "        # Prepare batch\n",
        "        rand_ind=np.random.randint(0,X_tr[0].size()[0],(int(par.batch_size/par.nClass),))\n",
        "        Im=X_tr[0][rand_ind,:]\n",
        "        Y=Y_tr[0][rand_ind,:]\n",
        "        for k in range(1,par.nClass):\n",
        "            rand_ind=np.random.randint(0,X_tr[k].size()[0],(int(par.batch_size/par.nClass),))\n",
        "            Im=torch.concat([Im,X_tr[k][rand_ind,:]],0)\n",
        "            Y=torch.concat([Y,Y_tr[k][rand_ind,:]],0)\n",
        "        \n",
        "        # Compute loss and accuracy\n",
        "        loss,Acc=MOD.response(Im.to(device), Y)    \n",
        "        L_tr[n]=np.copy(np.array(loss.to('cpu').detach()))\n",
        "        A_tr[n]=np.copy(np.array(Acc.to('cpu').detach()))\n",
        "        \n",
        "        ### Validation data\n",
        "        with torch.no_grad():\n",
        "            # Prepare batch\n",
        "            rand_ind=np.random.randint(0,X_val[0].size()[0],(int(par.batch_size/par.nClass),))\n",
        "            Im=X_val[0][rand_ind,:]\n",
        "            Y=Y_val[0][rand_ind,:]\n",
        "            for k in range(1,par.nClass):\n",
        "                rand_ind=np.random.randint(0,X_val[k].size()[0],(int(par.batch_size/par.nClass),))\n",
        "                Im=torch.concat([Im,X_val[k][rand_ind,:]],0)\n",
        "                Y=torch.concat([Y,Y_val[k][rand_ind,:]],0)\n",
        "            \n",
        "            # Compute loss and accuracy\n",
        "            loss,Acc=MOD.response(Im, Y, False)    \n",
        "            L_val[n]=np.copy(np.array(loss.to('cpu').detach()))\n",
        "            A_val[n]=np.copy(np.array(Acc.to('cpu').detach()))\n",
        "\n",
        "        # Update learning rate\n",
        "        MOD.opt.param_groups[0]['lr'] = par.eta * np.exp(-float(n)/par.eta_tau)\n",
        "\n",
        "        if (n%par.reportTime==0) and (n>0):\n",
        "            print(f'Time per stage: {time.time()-t}')\n",
        "            mseTr_mean=np.mean(L_tr[n-par.reportTime:n])\n",
        "            accTr_mean=np.mean(A_tr[n-par.reportTime:n])\n",
        "            mseVal_mean=np.mean(L_val[n-par.reportTime:n])\n",
        "            accVal_mean=np.mean(A_val[n-par.reportTime:n])\n",
        "            t=time.time()\n",
        "            \n",
        "            print(f'Progress: {np.float32(n)/np.float32(par.nEpochs)*100.:.3}%   Mean Tr Er: {mseTr_mean}, Mean Val Er: {mseVal_mean};   Mean Tr Acc: {accTr_mean}, Mean Val Acc: {accVal_mean}')\n",
        "\n",
        "    ### Save outputs\n",
        "    torch.save(L_tr, outputDir + '/' + 'lossTr'+str(rngSeed)+'.pt')\n",
        "    torch.save(A_tr, outputDir + '/' + 'accTr'+str(rngSeed)+'.pt')\n",
        "    torch.save(L_val, outputDir + '/' + 'lossVal'+str(rngSeed)+'.pt')\n",
        "    torch.save(A_val, outputDir + '/' + 'accVal'+str(rngSeed)+'.pt')\n",
        "    torch.save(RESP, outputDir + '/' + 'respSave'+str(rngSeed)+'.pt')\n",
        "    if par.saveFlag_FBWeights:\n",
        "        torch.save(savWeights, outputDir + '/' + 'weightSave'+str(rngSeed)+'.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "f140603c",
      "metadata": {},
      "outputs": [],
      "source": [
        "###\n",
        "### WITH metric learning\n",
        "###\n",
        "def metric_esn_fb(expName, rngSeed):\n",
        "    ### Setup directory names\n",
        "    experiment = expName\n",
        "    expDir = directory+'/data/'+experiment\n",
        "    if not os.path.exists(expDir):\n",
        "        os.mkdir(expDir)\n",
        "    outputDir = expDir    # Storage directory for input/label data\n",
        "    if not os.path.exists(outputDir):\n",
        "        os.mkdir(outputDir)\n",
        "\n",
        "    ### Other parameters\n",
        "    save_every = int(np.floor(par.nSaveMaxT / par.nSave)) # Save data every <> epochs, up to epoch nSaveMaxT\n",
        "    tMax = X_tr[0].shape[2] # No. time steps per input sequence\n",
        "\n",
        "    # Init memory to save responses\n",
        "    if par.saveFlag_RESP:\n",
        "        RESP = []\n",
        "        for layer in par.saveLayers:\n",
        "            RESP.append(np.zeros(( 10, par.nClass * par.nSaveSamples, par.Ns[layer], tMax)))\n",
        "    # Init memory to save feedback weights\n",
        "    if par.saveFlag_FBWeights:\n",
        "        print('Saving weights')\n",
        "        savWeights = np.zeros((par.Ns[par.fbLayer], par.Ns[0], par.nSave+1))\n",
        "    # Init memory to save effective learning rate\n",
        "    savDW = np.zeros(par.nEpochs)\n",
        "\n",
        "    ### Init RNG\n",
        "    torch.manual_seed(rngSeed)\n",
        "    np.random.seed(rngSeed)\n",
        "\n",
        "    ### Initialise model\n",
        "    MOD = MLPmetric(par)\n",
        "\n",
        "    MOD.Initialise_Hyperparameters(par.eta,par.batch_size)\n",
        "    L_tr=np.zeros([par.nEpochs])\n",
        "    A_tr=np.zeros([par.nEpochs])\n",
        "    L_val=np.zeros([par.nEpochs])\n",
        "    A_val=np.zeros([par.nEpochs])\n",
        "\n",
        "    print('**********************START TRAINING')\n",
        "    t=time.time()\n",
        "\n",
        "    for n in range(par.nEpochs):\n",
        "        ### Save met responses before updates\n",
        "        if n==(par.nSaveMaxT-1) and par.saveFlag_RESP:\n",
        "            print('Saving final response')\n",
        "            with torch.no_grad():\n",
        "                s = X_tr[0][0:par.nSaveSamples,:]\n",
        "                for k in range(1,par.nClass):\n",
        "                    s = torch.concat([s, X_tr[k][0:par.nSaveSamples,:]],0)\n",
        "                resp = MOD.responseSave(s, par.saveLayers)\n",
        "                for li in range(len(par.saveLayers)):\n",
        "                    RESP.append(resp[li].numpy())\n",
        "\n",
        "        ### Save feedback weights\n",
        "        if par.saveFlag_FBWeights:\n",
        "            if (n%save_every<1 and n<par.nSaveMaxT) or n==(par.nSaveMaxT-1):\n",
        "                print(f'Saving {int(n/save_every)+1} of {par.nSave+1}')\n",
        "                savWeights[:,:,int(np.floor(n/save_every))] = MOD.W_fb.data.cpu()\n",
        "\n",
        "        ### Training data\n",
        "        # Prepare batch\n",
        "        Im = torch.zeros([3*par.batch_size,par.nInputs,par.tMax]).to(device)\n",
        "        for k in range(par.nClass):\n",
        "            # Random indeces to select samples (2 for anchor and positive, then 1 for negative)\n",
        "            ind_ap = np.random.choice(X_tr[k].shape[0],(MOD.nSampPerClassPerBatch,2), replace=False)\n",
        "            # Populate Anchor and Positive samples\n",
        "            Im[k*MOD.nSampPerClassPerBatch:(k+1)*MOD.nSampPerClassPerBatch,:] = torch.clone(X_tr[k][ind_ap[:,0],:])\n",
        "            Im[par.batch_size+k*MOD.nSampPerClassPerBatch:par.batch_size+(k+1)*MOD.nSampPerClassPerBatch,:] = torch.clone(X_tr[k][ind_ap[:,1],:])\n",
        "            # Populate negative samples\n",
        "            randClass = np.random.choice((np.arange(par.nClass)!=k).nonzero()[0], MOD.nSampPerClassPerBatch)\n",
        "            for m, cl in enumerate(randClass):\n",
        "                Im[2*par.batch_size + k*MOD.nSampPerClassPerBatch+m,:] = torch.clone(X_tr[cl][np.random.randint(X_tr[cl].shape[0]),:])\n",
        "\n",
        "        # Compute loss\n",
        "        loss, Acc = MOD.response(Im, [], n)\n",
        "        #####\n",
        "        ##### CHECKING FOR NANS\n",
        "        if torch.any(torch.isnan(MOD.x[-1])):\n",
        "            print(f'Quitting at batch {n} of {par.nEpochs}')\n",
        "            return -1\n",
        "        #####\n",
        "        #####\n",
        "\n",
        "        # Store training loss and accuracy\n",
        "        L_tr[n]=np.copy(np.array(loss.to('cpu').detach()))\n",
        "        A_tr[n]=np.copy(np.array(Acc.to('cpu').detach()))\n",
        "        # Store effective rate change\n",
        "        dw = 0\n",
        "        for k in range(len(MOD.Ws)):\n",
        "            mask = torch.abs(MOD.Ws[k].grad) > 0\n",
        "            dw += torch.sum(torch.abs(MOD.Ws[k].grad)[mask]).cpu().numpy()\n",
        "        dw /= MOD.numW\n",
        "        savDW[n] = savDW[n] + np.divide(dw - savDW[n], n+1)\n",
        "\n",
        "        ### Validation data\n",
        "        with torch.no_grad():\n",
        "            # Prepare batch\n",
        "            Im = torch.zeros([3*par.batch_size,par.nInputs,par.tMax]).to(device)\n",
        "            # Y = torch.tile(torch.arange(par.nClass).unsqueeze(1), (1,nSampPerClassPerBatch)).reshape(batch_size)\n",
        "            for k in range(par.nClass):\n",
        "                # Random indeces to select samples (2 for anchor and positive, then 1 for negative)\n",
        "                ind_ap = np.random.choice(X_val[k].shape[0],(MOD.nSampPerClassPerBatch,2), replace=False)\n",
        "                # Populate Anchor and Positive samples\n",
        "                Im[k*MOD.nSampPerClassPerBatch:(k+1)*MOD.nSampPerClassPerBatch,:] = torch.clone(X_val[k][ind_ap[:,0],:])\n",
        "                Im[par.batch_size + k*MOD.nSampPerClassPerBatch:par.batch_size + (k+1)*MOD.nSampPerClassPerBatch,:] = torch.clone(X_val[k][ind_ap[:,1],:])\n",
        "                # Populate negative samples\n",
        "                randClass = np.random.choice((np.arange(par.nClass)!=k).nonzero()[0], MOD.nSampPerClassPerBatch)\n",
        "                for m, cl in enumerate(randClass):\n",
        "                    Im[2*par.batch_size + k*MOD.nSampPerClassPerBatch+m,:] = torch.clone(X_val[cl][np.random.randint(X_val[cl].shape[0]),:])\n",
        "\n",
        "            # Compute loss and accuracy\n",
        "            loss,Acc=MOD.response(Im, [], n, False)\n",
        "            L_val[n]=np.copy(np.array(loss.to('cpu').detach()))\n",
        "            A_val[n]=np.copy(np.array(Acc.to('cpu').detach()))\n",
        "\n",
        "        # Update learning rate\n",
        "        MOD.opt.param_groups[0]['lr'] = par.eta * np.exp(-float(n)/par.eta_tau)\n",
        "\n",
        "        if (n%par.reportTime==0) and (n>0):\n",
        "            print(f'Time per stage: {time.time()-t}')\n",
        "            mseTr_mean=np.mean(L_tr[n-par.reportTime:n])\n",
        "            accTr_mean=np.mean(A_tr[n-par.reportTime:n])\n",
        "            mseVal_mean=np.mean(L_val[n-par.reportTime:n])\n",
        "            accVal_mean=np.mean(A_val[n-par.reportTime:n])\n",
        "            dw_mean=np.mean(savDW[n-par.reportTime:n])\n",
        "            t=time.time()\n",
        "\n",
        "            print(f'Progress: {np.float32(n)/np.float32(par.nEpochs)*100.:.3}%   Mean Tr Er: {mseTr_mean}, Mean Val Er: {mseVal_mean};   Mean Tr Acc: {accTr_mean}, Mean Val Acc: {accVal_mean}, DW: {dw_mean}')\n",
        "\n",
        "    ### Save outputs\n",
        "    torch.save(L_tr, outputDir + '/' + 'lossTr'+str(rngSeed)+'.pt')\n",
        "    torch.save(A_tr, outputDir + '/' + 'accTr'+str(rngSeed)+'.pt')\n",
        "    torch.save(L_val, outputDir + '/' + 'lossVal'+str(rngSeed)+'.pt')\n",
        "    torch.save(A_val, outputDir + '/' + 'accVal'+str(rngSeed)+'.pt')\n",
        "    torch.save(RESP, outputDir + '/' + 'respSave'+str(rngSeed)+'.pt')\n",
        "    torch.save(MOD.W, outputDir + '/' + 'Wesn'+str(rngSeed)+'.pt')\n",
        "    torch.save(savDW, outputDir + '/' + 'dw'+str(rngSeed)+'.pt')\n",
        "    if par.saveFlag_FBWeights:\n",
        "        torch.save(savWeights, outputDir + '/' + 'weightSave'+str(rngSeed)+'.pt')\n",
        "\n",
        "    return 1\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "fa8aa35e",
      "metadata": {},
      "source": [
        "# Run parameter sweep (CrossEntropy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ad44485",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Define parameters\n",
        "swLR = np.logspace(-5,-2,10) # Sweep over these learning rates\n",
        "nSeeds = 5 # No. runs per hyperparameter\n",
        "seeds = list(sp.primerange(11111,33333))[0:nSeeds] # RNG seeds\n",
        "for j, lr in enumerate(swLR):\n",
        "    for k, sd in enumerate(seeds):\n",
        "        # Init RNG\n",
        "        torch.manual_seed(sd)\n",
        "        np.random.default_rng(sd)\n",
        "        # Update hyperparameter\n",
        "        importlib.reload(par)\n",
        "        par.eta = swLR[j]\n",
        "        par.fbLayer = 3\n",
        "        par.saveFlag_FBWeights = False if not par.fbLayer else True \n",
        "        expName = 'FB0_swLR_'+str(j)\n",
        "        xent_esn_fb(expName, sd)\n",
        "        print(f'j={j}/{len(swLR)};     k={k}/{len(seeds)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e95e7dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "swLR = 0.001#0.00046 # Sweep over these learning rates\n",
        "seed = 11117 # No. runs per hyperparameter\n",
        "\n",
        "# Update hyperparameter\n",
        "importlib.reload(par)\n",
        "par.eta = swLR\n",
        "par.fbLayer = 3\n",
        "par.saveFlag_FBWeights = True\n",
        "expName = 'met_FB3_swLR_9'\n",
        "metric_esn_fb(expName, seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "c625fc92",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving weights\n",
            "**********************START TRAINING\n",
            "Saving 1 of 26\n",
            "DP = 0.06169263273477554                            DN = 0.07945370674133301                           maxRESP = 0.11279083043336868\n",
            "DP = 0.06274572759866714                            DN = 0.08900298923254013                           maxRESP = 0.14440789818763733\n",
            "DP = 0.07164812088012695                            DN = 0.12082605808973312                           maxRESP = 0.1658070683479309\n",
            "DP = 0.08143135905265808                            DN = 0.1084008440375328                           maxRESP = 0.1612449437379837\n",
            "DP = 0.1424212008714676                            DN = 0.17532972991466522                           maxRESP = 0.22787228226661682\n",
            "DP = 0.12211696058511734                            DN = 0.21137073636054993                           maxRESP = 0.22413434088230133\n",
            "DP = 0.13188141584396362                            DN = 0.24620261788368225                           maxRESP = 0.360511839389801\n",
            "DP = 0.1372671127319336                            DN = 0.2661695182323456                           maxRESP = 0.331641286611557\n",
            "Saving 2 of 26\n",
            "DP = 0.19186446070671082                            DN = 0.30594927072525024                           maxRESP = 0.382282018661499\n",
            "DP = 0.20302334427833557                            DN = 0.29930081963539124                           maxRESP = 0.43880683183670044\n",
            "DP = 0.27878865599632263                            DN = 0.5158082842826843                           maxRESP = 0.6233497262001038\n",
            "DP = 0.2264128178358078                            DN = 0.39917051792144775                           maxRESP = 0.6543087363243103\n",
            "Time per stage: 11.55587387084961\n",
            "Progress: 5.0%   Mean Tr Er: 1.9239819684028625, Mean Val Er: 1.9246819615364075;   Mean Tr Acc: 0.6389000105857849, Mean Val Acc: 0.6517000119686127, DW: 2.2835993492582236e-05\n",
            "DP = 0.28217804431915283                            DN = 0.609244704246521                           maxRESP = 0.703628420829773\n",
            "DP = 0.2752356231212616                            DN = 0.5533379912376404                           maxRESP = 0.7389628887176514\n",
            "DP = 0.3689992129802704                            DN = 0.8132266998291016                           maxRESP = 0.8967806100845337\n",
            "DP = 0.35509398579597473                            DN = 0.9182578325271606                           maxRESP = 0.9971347451210022\n",
            "Saving 3 of 26\n",
            "DP = 0.5633494257926941                            DN = 1.3258408308029175                           maxRESP = 1.5715185403823853\n",
            "DP = 0.5468798875808716                            DN = 1.2787435054779053                           maxRESP = 1.711351752281189\n",
            "DP = 0.7012424468994141                            DN = 1.5706347227096558                           maxRESP = 1.774230718612671\n",
            "DP = 0.8897972106933594                            DN = 1.8167498111724854                           maxRESP = 2.295877695083618\n",
            "DP = 0.7433637976646423                            DN = 1.9084080457687378                           maxRESP = 2.529604434967041\n",
            "DP = 0.7038801312446594                            DN = 1.7492855787277222                           maxRESP = 2.293915033340454\n",
            "Time per stage: 11.331142902374268\n",
            "Progress: 10.0%   Mean Tr Er: 1.6018615951538087, Mean Val Er: 1.5942952282428742;   Mean Tr Acc: 0.5874000117778778, Mean Val Acc: 0.5914000121355056, DW: 4.511562240173834e-06\n",
            "DP = 0.7113053798675537                            DN = 1.7100578546524048                           maxRESP = 2.1005239486694336\n",
            "DP = 0.6646947264671326                            DN = 2.0078322887420654                           maxRESP = 2.307511568069458\n",
            "Saving 4 of 26\n",
            "DP = 0.9719167947769165                            DN = 2.159280300140381                           maxRESP = 2.444403886795044\n",
            "DP = 0.9643463492393494                            DN = 2.1750831604003906                           maxRESP = 2.3277971744537354\n",
            "DP = 1.1370850801467896                            DN = 2.182790517807007                           maxRESP = 2.7043089866638184\n",
            "DP = 1.1926367282867432                            DN = 2.4858040809631348                           maxRESP = 2.696063995361328\n",
            "DP = 1.4392926692962646                            DN = 2.3011279106140137                           maxRESP = 2.7882144451141357\n",
            "DP = 1.2782872915267944                            DN = 2.9926209449768066                           maxRESP = 3.014873504638672\n",
            "DP = 1.3862327337265015                            DN = 2.825623035430908                           maxRESP = 2.959392547607422\n",
            "DP = 1.2801541090011597                            DN = 2.6337544918060303                           maxRESP = 3.0980637073516846\n",
            "Time per stage: 11.32387661933899\n",
            "Progress: 15.0%   Mean Tr Er: 1.210310343503952, Mean Val Er: 1.1982150242328644;   Mean Tr Acc: 0.6140000112056733, Mean Val Acc: 0.6117000118494034, DW: 3.6300100907329778e-06\n",
            "Saving 5 of 26\n",
            "DP = 1.2238126993179321                            DN = 2.960301160812378                           maxRESP = 3.4378726482391357\n",
            "DP = 1.2770143747329712                            DN = 3.0367343425750732                           maxRESP = 3.3789072036743164\n",
            "DP = 1.2485724687576294                            DN = 3.2454617023468018                           maxRESP = 3.397190570831299\n",
            "DP = 1.3188762664794922                            DN = 2.810490131378174                           maxRESP = 3.2107675075531006\n",
            "DP = 1.2461727857589722                            DN = 2.9840362071990967                           maxRESP = 3.56889009475708\n",
            "DP = 1.3005589246749878                            DN = 3.297217607498169                           maxRESP = 3.4160916805267334\n",
            "DP = 1.4062455892562866                            DN = 3.038029670715332                           maxRESP = 3.788111686706543\n",
            "DP = 1.4330867528915405                            DN = 2.948817253112793                           maxRESP = 3.6831395626068115\n",
            "Saving 6 of 26\n",
            "DP = 1.2804499864578247                            DN = 3.316148519515991                           maxRESP = 3.798703908920288\n",
            "DP = 1.2756744623184204                            DN = 3.3055481910705566                           maxRESP = 3.774151086807251\n",
            "Time per stage: 11.46196985244751\n",
            "Progress: 20.0%   Mean Tr Er: 1.0951322116851807, Mean Val Er: 1.0696016627550125;   Mean Tr Acc: 0.6464000115394593, Mean Val Acc: 0.6496000100374222, DW: 2.8924716653741114e-06\n",
            "DP = 1.4990252256393433                            DN = 3.4658150672912598                           maxRESP = 3.507004737854004\n",
            "DP = 1.2741435766220093                            DN = 3.0657882690429688                           maxRESP = 4.1696319580078125\n",
            "DP = 1.2885140180587769                            DN = 3.303853750228882                           maxRESP = 4.457581996917725\n",
            "DP = 1.1911872625350952                            DN = 3.0706489086151123                           maxRESP = 3.79406476020813\n",
            "DP = 1.5492172241210938                            DN = 3.251866102218628                           maxRESP = 4.659692287445068\n",
            "DP = 1.2515918016433716                            DN = 3.234792709350586                           maxRESP = 4.932490825653076\n",
            "Saving 7 of 26\n",
            "DP = 1.3591890335083008                            DN = 3.6049256324768066                           maxRESP = 4.198048114776611\n",
            "DP = 1.2178146839141846                            DN = 3.4713501930236816                           maxRESP = 3.787407159805298\n",
            "DP = 1.4498118162155151                            DN = 3.710906982421875                           maxRESP = 4.234529972076416\n",
            "DP = 1.4418843984603882                            DN = 3.359632968902588                           maxRESP = 4.024040222167969\n",
            "Time per stage: 11.485639333724976\n",
            "Progress: 25.0%   Mean Tr Er: 1.0375436012744903, Mean Val Er: 1.0099421175718308;   Mean Tr Acc: 0.6680000109672546, Mean Val Acc: 0.6761000107526779, DW: 2.3080221441047297e-06\n",
            "DP = 1.3791805505752563                            DN = 3.095846652984619                           maxRESP = 5.157556533813477\n",
            "DP = 1.1781141757965088                            DN = 3.7219064235687256                           maxRESP = 4.5731587409973145\n",
            "DP = 1.393670916557312                            DN = 3.463303327560425                           maxRESP = 3.9666600227355957\n",
            "DP = 1.445110559463501                            DN = 2.884181499481201                           maxRESP = 3.7040302753448486\n",
            "Saving 8 of 26\n",
            "DP = 1.6882027387619019                            DN = 3.6342034339904785                           maxRESP = 3.9641594886779785\n",
            "DP = 1.2818087339401245                            DN = 3.1085917949676514                           maxRESP = 4.026258945465088\n",
            "DP = 1.3166790008544922                            DN = 3.1646690368652344                           maxRESP = 4.747995853424072\n",
            "DP = 1.562665581703186                            DN = 3.505201816558838                           maxRESP = 4.514301300048828\n",
            "DP = 1.55130934715271                            DN = 3.1478378772735596                           maxRESP = 4.551191806793213\n",
            "DP = 1.4308212995529175                            DN = 3.2171390056610107                           maxRESP = 4.5471930503845215\n",
            "Time per stage: 11.699662923812866\n",
            "Progress: 30.0%   Mean Tr Er: 0.9663237819671631, Mean Val Er: 0.9444780668020248;   Mean Tr Acc: 0.6935000109672547, Mean Val Acc: 0.6899000110626221, DW: 1.9310529830637712e-06\n",
            "DP = 1.8358185291290283                            DN = 3.5166375637054443                           maxRESP = 4.928338527679443\n",
            "DP = 1.5379457473754883                            DN = 3.958876132965088                           maxRESP = 4.314811706542969\n",
            "Saving 9 of 26\n",
            "DP = 1.331238031387329                            DN = 3.9622113704681396                           maxRESP = 4.81227445602417\n",
            "DP = 1.5161694288253784                            DN = 3.6683189868927                           maxRESP = 4.998786926269531\n",
            "DP = 1.5472387075424194                            DN = 3.165393590927124                           maxRESP = 4.651698112487793\n",
            "DP = 1.5128637552261353                            DN = 3.821397542953491                           maxRESP = 5.393752574920654\n",
            "DP = 1.3601945638656616                            DN = 3.1767284870147705                           maxRESP = 4.670949935913086\n",
            "DP = 1.3011995553970337                            DN = 3.6724190711975098                           maxRESP = 4.926351070404053\n",
            "DP = 1.5486478805541992                            DN = 3.1166627407073975                           maxRESP = 4.724948406219482\n",
            "DP = 1.3497154712677002                            DN = 3.5941693782806396                           maxRESP = 5.064505100250244\n",
            "Time per stage: 11.4052095413208\n",
            "Progress: 35.0%   Mean Tr Er: 0.9430770251750946, Mean Val Er: 0.9192042150497437;   Mean Tr Acc: 0.7004000127315522, Mean Val Acc: 0.7057000105381012, DW: 1.7307403994571415e-06\n",
            "Saving 10 of 26\n",
            "DP = 1.5537041425704956                            DN = 3.6821353435516357                           maxRESP = 4.469447135925293\n",
            "DP = 1.430557131767273                            DN = 3.604818105697632                           maxRESP = 4.684715270996094\n",
            "DP = 1.5875554084777832                            DN = 3.754148483276367                           maxRESP = 4.572578430175781\n",
            "DP = 1.5865813493728638                            DN = 3.70469069480896                           maxRESP = 4.466300010681152\n",
            "DP = 1.46015465259552                            DN = 3.2374749183654785                           maxRESP = 5.336040019989014\n",
            "DP = 1.5558265447616577                            DN = 3.7550675868988037                           maxRESP = 4.453496932983398\n",
            "DP = 1.6302807331085205                            DN = 3.790649890899658                           maxRESP = 4.896257400512695\n",
            "DP = 1.5192139148712158                            DN = 3.8995025157928467                           maxRESP = 4.94485330581665\n",
            "Saving 11 of 26\n",
            "DP = 1.5659929513931274                            DN = 3.9437615871429443                           maxRESP = 4.674572944641113\n",
            "DP = 1.6489193439483643                            DN = 4.5318522453308105                           maxRESP = 5.506260871887207\n",
            "Time per stage: 11.312735319137573\n",
            "Progress: 40.0%   Mean Tr Er: 0.8998341997861862, Mean Val Er: 0.9000650947093963;   Mean Tr Acc: 0.7189000132083893, Mean Val Acc: 0.7176000103950501, DW: 1.549357864050024e-06\n",
            "DP = 1.3964725732803345                            DN = 4.269583225250244                           maxRESP = 5.379638671875\n",
            "DP = 1.6548740863800049                            DN = 4.014639854431152                           maxRESP = 4.472987174987793\n",
            "DP = 1.5543864965438843                            DN = 3.7269504070281982                           maxRESP = 5.421903610229492\n",
            "DP = 1.5671045780181885                            DN = 4.172744274139404                           maxRESP = 4.925955772399902\n",
            "DP = 1.4357550144195557                            DN = 3.8558907508850098                           maxRESP = 5.134905815124512\n",
            "DP = 1.7128759622573853                            DN = 3.9894814491271973                           maxRESP = 4.708178520202637\n",
            "Saving 12 of 26\n",
            "DP = 1.367073655128479                            DN = 3.8570504188537598                           maxRESP = 4.758342742919922\n",
            "DP = 1.2786967754364014                            DN = 4.101685523986816                           maxRESP = 4.494105815887451\n",
            "DP = 1.599137544631958                            DN = 3.9592607021331787                           maxRESP = 4.846097469329834\n",
            "DP = 1.2643539905548096                            DN = 3.8310093879699707                           maxRESP = 4.922882080078125\n",
            "Time per stage: 11.256667375564575\n",
            "Progress: 45.0%   Mean Tr Er: 0.8695506768226624, Mean Val Er: 0.880997149348259;   Mean Tr Acc: 0.7248000087738037, Mean Val Acc: 0.7254000115394592, DW: 1.3938745122596596e-06\n",
            "DP = 1.5278210639953613                            DN = 4.3592047691345215                           maxRESP = 5.306272983551025\n",
            "DP = 1.3268834352493286                            DN = 3.740934133529663                           maxRESP = 5.031221389770508\n",
            "DP = 1.5725334882736206                            DN = 3.4534928798675537                           maxRESP = 4.788382053375244\n",
            "DP = 1.5242902040481567                            DN = 4.023751735687256                           maxRESP = 4.50505256652832\n",
            "Saving 13 of 26\n",
            "DP = 1.4053400754928589                            DN = 3.8018760681152344                           maxRESP = 4.596998691558838\n",
            "DP = 1.7267639636993408                            DN = 4.217967510223389                           maxRESP = 4.552618980407715\n",
            "DP = 1.571224570274353                            DN = 3.946134567260742                           maxRESP = 4.8967413902282715\n",
            "DP = 1.5670140981674194                            DN = 3.827599287033081                           maxRESP = 5.314393997192383\n",
            "DP = 1.3504393100738525                            DN = 4.22217321395874                           maxRESP = 5.2631611824035645\n",
            "DP = 1.837952971458435                            DN = 3.914926290512085                           maxRESP = 5.59549617767334\n",
            "Time per stage: 11.827759265899658\n",
            "Progress: 50.0%   Mean Tr Er: 0.8230190782546997, Mean Val Er: 0.8525428496599198;   Mean Tr Acc: 0.7391000126600266, Mean Val Acc: 0.7367000124454498, DW: 1.2814352282156267e-06\n",
            "DP = 1.7132676839828491                            DN = 4.089391231536865                           maxRESP = 4.920768737792969\n",
            "DP = 1.4907363653182983                            DN = 3.783377170562744                           maxRESP = 4.815885066986084\n",
            "Saving 14 of 26\n",
            "DP = 1.4590696096420288                            DN = 4.040582656860352                           maxRESP = 5.513667106628418\n",
            "DP = 1.6226425170898438                            DN = 4.050164699554443                           maxRESP = 4.72551155090332\n",
            "DP = 1.7583659887313843                            DN = 4.139379978179932                           maxRESP = 4.846223831176758\n",
            "DP = 1.3739099502563477                            DN = 3.971019744873047                           maxRESP = 5.136229038238525\n",
            "DP = 1.6294876337051392                            DN = 3.9008445739746094                           maxRESP = 4.871932029724121\n",
            "DP = 1.5198583602905273                            DN = 4.2037811279296875                           maxRESP = 5.068284034729004\n",
            "DP = 1.6608072519302368                            DN = 4.131852149963379                           maxRESP = 4.892994403839111\n",
            "DP = 1.749942421913147                            DN = 3.705061435699463                           maxRESP = 5.325585842132568\n",
            "Time per stage: 11.37207579612732\n",
            "Progress: 55.0%   Mean Tr Er: 0.8306813569068908, Mean Val Er: 0.7970595570802689;   Mean Tr Acc: 0.7528000106811523, Mean Val Acc: 0.7477000112533569, DW: 1.2211560459198769e-06\n",
            "Saving 15 of 26\n",
            "DP = 1.4969112873077393                            DN = 4.021708965301514                           maxRESP = 5.400079250335693\n",
            "DP = 1.2088197469711304                            DN = 4.559415340423584                           maxRESP = 5.176112651824951\n",
            "DP = 1.4869043827056885                            DN = 4.086803913116455                           maxRESP = 4.921076774597168\n",
            "DP = 1.4926789999008179                            DN = 4.1353254318237305                           maxRESP = 5.309235572814941\n",
            "DP = 1.5755109786987305                            DN = 4.0614542961120605                           maxRESP = 5.064728260040283\n",
            "DP = 1.7873414754867554                            DN = 4.029097080230713                           maxRESP = 4.953248500823975\n",
            "DP = 1.653760552406311                            DN = 4.391448497772217                           maxRESP = 5.132195472717285\n",
            "DP = 1.6906226873397827                            DN = 3.5965542793273926                           maxRESP = 6.302535533905029\n",
            "Saving 16 of 26\n",
            "DP = 1.4955953359603882                            DN = 4.375507354736328                           maxRESP = 5.774322509765625\n",
            "DP = 1.417446255683899                            DN = 4.321794033050537                           maxRESP = 5.253104209899902\n",
            "Time per stage: 11.300318241119385\n",
            "Progress: 60.0%   Mean Tr Er: 0.780753269314766, Mean Val Er: 0.7827329083681106;   Mean Tr Acc: 0.7684000120162964, Mean Val Acc: 0.7623000133037567, DW: 1.1121846713359266e-06\n",
            "DP = 1.6656776666641235                            DN = 4.411496639251709                           maxRESP = 5.251345634460449\n",
            "DP = 1.5762802362442017                            DN = 4.33966588973999                           maxRESP = 5.457630157470703\n",
            "DP = 1.3714008331298828                            DN = 4.07876443862915                           maxRESP = 4.899987697601318\n",
            "DP = 1.5920984745025635                            DN = 4.363353252410889                           maxRESP = 5.515153884887695\n",
            "DP = 1.9486305713653564                            DN = 3.742114305496216                           maxRESP = 5.991125583648682\n",
            "DP = 1.8358558416366577                            DN = 4.335507869720459                           maxRESP = 5.583721160888672\n",
            "Saving 17 of 26\n",
            "DP = 1.4415016174316406                            DN = 4.122816562652588                           maxRESP = 6.034367084503174\n",
            "DP = 1.6471004486083984                            DN = 4.449104309082031                           maxRESP = 5.37813138961792\n",
            "DP = 1.5575164556503296                            DN = 4.182061195373535                           maxRESP = 5.500782012939453\n",
            "DP = 1.579727292060852                            DN = 3.9490203857421875                           maxRESP = 5.32558012008667\n",
            "Time per stage: 11.303053140640259\n",
            "Progress: 65.0%   Mean Tr Er: 0.761928729057312, Mean Val Er: 0.7573573632836342;   Mean Tr Acc: 0.7726000106334686, Mean Val Acc: 0.774300011396408, DW: 1.0656619714988556e-06\n",
            "DP = 1.5559076070785522                            DN = 3.93756103515625                           maxRESP = 5.034475326538086\n",
            "DP = 1.7792338132858276                            DN = 4.391063690185547                           maxRESP = 5.467718601226807\n",
            "DP = 1.7506083250045776                            DN = 4.516491889953613                           maxRESP = 5.569140911102295\n",
            "DP = 1.656882882118225                            DN = 4.420962810516357                           maxRESP = 5.989499092102051\n",
            "Saving 18 of 26\n",
            "DP = 1.677868127822876                            DN = 4.45348596572876                           maxRESP = 5.858458042144775\n",
            "DP = 1.4687752723693848                            DN = 4.315118312835693                           maxRESP = 5.756198883056641\n",
            "DP = 1.704161524772644                            DN = 4.223802089691162                           maxRESP = 5.297084808349609\n",
            "DP = 1.7654746770858765                            DN = 4.665374755859375                           maxRESP = 5.5303263664245605\n",
            "DP = 1.487282156944275                            DN = 4.534554958343506                           maxRESP = 5.286261558532715\n",
            "DP = 1.493030071258545                            DN = 4.2372918128967285                           maxRESP = 5.419278144836426\n",
            "Time per stage: 11.400224924087524\n",
            "Progress: 70.0%   Mean Tr Er: 0.7368745305538178, Mean Val Er: 0.7351447174549103;   Mean Tr Acc: 0.7786000106334686, Mean Val Acc: 0.7740000128746033, DW: 1.008091895349732e-06\n",
            "DP = 1.6983680725097656                            DN = 4.805995941162109                           maxRESP = 5.498717308044434\n",
            "DP = 1.7663246393203735                            DN = 4.015477657318115                           maxRESP = 5.47344446182251\n",
            "Saving 19 of 26\n",
            "DP = 1.8012539148330688                            DN = 4.42822265625                           maxRESP = 5.9351019859313965\n",
            "DP = 1.5581209659576416                            DN = 4.2855095863342285                           maxRESP = 5.777130126953125\n",
            "DP = 1.8924983739852905                            DN = 4.249019145965576                           maxRESP = 5.700151443481445\n",
            "DP = 1.748769998550415                            DN = 4.142076015472412                           maxRESP = 5.985502243041992\n",
            "DP = 1.494248390197754                            DN = 4.261029243469238                           maxRESP = 5.765234470367432\n",
            "DP = 1.7018934488296509                            DN = 4.505599021911621                           maxRESP = 5.937074661254883\n",
            "DP = 1.5192956924438477                            DN = 4.6524505615234375                           maxRESP = 5.814764499664307\n",
            "DP = 1.5343502759933472                            DN = 4.145534038543701                           maxRESP = 5.393806457519531\n",
            "Time per stage: 11.462432384490967\n",
            "Progress: 75.0%   Mean Tr Er: 0.7190927245616913, Mean Val Er: 0.7053673679828644;   Mean Tr Acc: 0.7808000118732452, Mean Val Acc: 0.7827000124454498, DW: 9.699239916823982e-07\n",
            "Saving 20 of 26\n",
            "DP = 1.6585180759429932                            DN = 4.378878116607666                           maxRESP = 5.808929920196533\n",
            "DP = 1.6449002027511597                            DN = 4.262557506561279                           maxRESP = 5.992282867431641\n",
            "DP = 1.8137654066085815                            DN = 4.458347320556641                           maxRESP = 6.268276214599609\n",
            "DP = 1.9982086420059204                            DN = 4.071285724639893                           maxRESP = 5.977471828460693\n",
            "DP = 1.6459732055664062                            DN = 4.794801235198975                           maxRESP = 5.859466552734375\n",
            "DP = 1.685640573501587                            DN = 4.305156707763672                           maxRESP = 5.788040637969971\n",
            "DP = 1.3822486400604248                            DN = 4.492135047912598                           maxRESP = 5.70481538772583\n",
            "DP = 1.7617288827896118                            DN = 4.742753028869629                           maxRESP = 6.541943073272705\n",
            "Saving 21 of 26\n",
            "DP = 1.4015830755233765                            DN = 4.3373284339904785                           maxRESP = 6.570007801055908\n",
            "DP = 1.6966447830200195                            DN = 4.450843334197998                           maxRESP = 5.919493675231934\n",
            "Time per stage: 11.548948049545288\n",
            "Progress: 80.0%   Mean Tr Er: 0.6986451591253281, Mean Val Er: 0.696447242796421;   Mean Tr Acc: 0.7954000115394593, Mean Val Acc: 0.7935000123977661, DW: 9.361113937494975e-07\n",
            "DP = 1.78817617893219                            DN = 4.064759731292725                           maxRESP = 6.0617876052856445\n",
            "DP = 1.6765727996826172                            DN = 4.38223123550415                           maxRESP = 5.920559883117676\n",
            "DP = 1.6298977136611938                            DN = 4.9444756507873535                           maxRESP = 5.731085777282715\n",
            "DP = 1.6982945203781128                            DN = 4.9720306396484375                           maxRESP = 6.685298442840576\n",
            "DP = 1.7341225147247314                            DN = 4.41694974899292                           maxRESP = 5.4562087059021\n",
            "DP = 1.9413689374923706                            DN = 4.222926616668701                           maxRESP = 6.611758232116699\n",
            "Saving 22 of 26\n",
            "DP = 1.80663001537323                            DN = 4.545161247253418                           maxRESP = 6.065101623535156\n",
            "DP = 1.673794150352478                            DN = 4.552708625793457                           maxRESP = 5.910798072814941\n",
            "DP = 1.7247692346572876                            DN = 4.994914531707764                           maxRESP = 5.802444934844971\n",
            "DP = 1.757659912109375                            DN = 4.476353645324707                           maxRESP = 5.179370403289795\n",
            "Time per stage: 11.498436212539673\n",
            "Progress: 85.0%   Mean Tr Er: 0.6875736396312714, Mean Val Er: 0.6986344217061996;   Mean Tr Acc: 0.7863000102043152, Mean Val Acc: 0.7970000114440918, DW: 9.179947268335722e-07\n",
            "DP = 1.4196722507476807                            DN = 4.49409818649292                           maxRESP = 6.2003173828125\n",
            "DP = 1.8839048147201538                            DN = 4.575637340545654                           maxRESP = 5.884243488311768\n",
            "DP = 1.5816380977630615                            DN = 4.371586799621582                           maxRESP = 6.3106207847595215\n",
            "DP = 1.6650348901748657                            DN = 4.466567516326904                           maxRESP = 6.228616714477539\n",
            "Saving 23 of 26\n",
            "DP = 1.9461525678634644                            DN = 4.866024017333984                           maxRESP = 6.462364196777344\n",
            "DP = 1.5480717420578003                            DN = 4.418288707733154                           maxRESP = 6.378487586975098\n",
            "DP = 1.579581618309021                            DN = 4.915451526641846                           maxRESP = 6.2982025146484375\n",
            "DP = 1.6781774759292603                            DN = 4.545513153076172                           maxRESP = 6.646970748901367\n",
            "DP = 1.5672930479049683                            DN = 4.413793087005615                           maxRESP = 5.702186584472656\n",
            "DP = 1.603471040725708                            DN = 4.605241298675537                           maxRESP = 6.237830638885498\n",
            "Time per stage: 11.614922285079956\n",
            "Progress: 90.0%   Mean Tr Er: 0.6721101323366165, Mean Val Er: 0.6789958677887916;   Mean Tr Acc: 0.7930000109672546, Mean Val Acc: 0.8019000134468078, DW: 8.598169843398153e-07\n",
            "DP = 1.8957818746566772                            DN = 4.62335729598999                           maxRESP = 6.862614154815674\n",
            "DP = 2.2130515575408936                            DN = 4.842700958251953                           maxRESP = 7.125922203063965\n",
            "Saving 24 of 26\n",
            "DP = 1.7876207828521729                            DN = 4.727065086364746                           maxRESP = 5.97755241394043\n",
            "DP = 1.7795555591583252                            DN = 4.580191612243652                           maxRESP = 6.534268856048584\n",
            "DP = 1.5399411916732788                            DN = 4.591223239898682                           maxRESP = 6.153605937957764\n",
            "DP = 1.5716108083724976                            DN = 4.390960693359375                           maxRESP = 6.176935195922852\n",
            "DP = 1.7493860721588135                            DN = 4.851253032684326                           maxRESP = 5.763821125030518\n",
            "DP = 1.552832007408142                            DN = 4.6601691246032715                           maxRESP = 6.393032073974609\n",
            "DP = 1.9494987726211548                            DN = 4.632806777954102                           maxRESP = 6.541957378387451\n",
            "DP = 1.5108896493911743                            DN = 4.769706726074219                           maxRESP = 6.495414733886719\n",
            "Time per stage: 11.281062602996826\n",
            "Progress: 95.0%   Mean Tr Er: 0.6500487173199654, Mean Val Er: 0.6606836546063423;   Mean Tr Acc: 0.8066000113487244, Mean Val Acc: 0.8044000117778778, DW: 8.186214668055366e-07\n",
            "Saving 25 of 26\n",
            "DP = 1.8205091953277588                            DN = 4.539628505706787                           maxRESP = 7.162020206451416\n",
            "DP = 1.8123953342437744                            DN = 5.268947601318359                           maxRESP = 7.012784957885742\n",
            "DP = 1.8515650033950806                            DN = 4.315533638000488                           maxRESP = 5.98492956161499\n",
            "DP = 1.5563656091690063                            DN = 4.51752233505249                           maxRESP = 6.733624458312988\n",
            "DP = 1.5088489055633545                            DN = 4.686797618865967                           maxRESP = 6.281103610992432\n",
            "DP = 2.0365922451019287                            DN = 4.392871379852295                           maxRESP = 6.300634384155273\n",
            "DP = 1.4783531427383423                            DN = 4.830119609832764                           maxRESP = 5.446321964263916\n",
            "DP = 1.8733128309249878                            DN = 5.007390022277832                           maxRESP = 5.845490455627441\n",
            "Saving final response\n",
            "Saving 25 of 26\n",
            "j=0/9;     k=0/5\n",
            "*************************Run time: 230.52709579467773\n",
            "Saving weights\n",
            "**********************START TRAINING\n",
            "Saving 1 of 26\n",
            "DP = 0.0820012018084526                            DN = 0.1115180253982544                           maxRESP = 0.1541375070810318\n",
            "DP = 0.07320483028888702                            DN = 0.09139034897089005                           maxRESP = 0.16018517315387726\n",
            "DP = 0.09294714778661728                            DN = 0.1539522111415863                           maxRESP = 0.24255461990833282\n",
            "DP = 0.1130477711558342                            DN = 0.17053338885307312                           maxRESP = 0.18581470847129822\n",
            "DP = 0.11795621365308762                            DN = 0.21755483746528625                           maxRESP = 0.22938911616802216\n",
            "DP = 0.12773972749710083                            DN = 0.23984554409980774                           maxRESP = 0.28754541277885437\n",
            "DP = 0.14219406247138977                            DN = 0.27433493733406067                           maxRESP = 0.41821473836898804\n",
            "DP = 0.13812391459941864                            DN = 0.31807711720466614                           maxRESP = 0.3151892423629761\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/its/home/jb739/esn2sparse/esn_feedback.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn2sparse/esn_feedback.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m par\u001b[39m.\u001b[39msaveFlag_FBWeights \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m par\u001b[39m.\u001b[39mfbLayer \u001b[39melse\u001b[39;00m \u001b[39mTrue\u001b[39;00m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn2sparse/esn_feedback.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m expName \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmet_FB1_swLR_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(j)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn2sparse/esn_feedback.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m complete[j, k] \u001b[39m=\u001b[39m metric_esn_fb(expName, sd)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn2sparse/esn_feedback.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mj=\u001b[39m\u001b[39m{\u001b[39;00mj\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(swLR)\u001b[39m}\u001b[39;00m\u001b[39m;     k=\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(seeds)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn2sparse/esn_feedback.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m*************************Run time: \u001b[39m\u001b[39m{\u001b[39;00mtime\u001b[39m.\u001b[39mtime()\u001b[39m-\u001b[39mt\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;32m/its/home/jb739/esn2sparse/esn_feedback.ipynb Cell 19\u001b[0m in \u001b[0;36mmetric_esn_fb\u001b[0;34m(expName, rngSeed)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn2sparse/esn_feedback.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m         Im[\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mpar\u001b[39m.\u001b[39mbatch_size \u001b[39m+\u001b[39m k\u001b[39m*\u001b[39mMOD\u001b[39m.\u001b[39mnSampPerClassPerBatch\u001b[39m+\u001b[39mm,:] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclone(X_tr[cl][np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(X_tr[cl]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]),:])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn2sparse/esn_feedback.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39m# Compute loss\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn2sparse/esn_feedback.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m loss, Acc \u001b[39m=\u001b[39m MOD\u001b[39m.\u001b[39;49mresponse(Im, [], n)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn2sparse/esn_feedback.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39m#####\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn2sparse/esn_feedback.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39m##### CHECKING FOR NANS\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn2sparse/esn_feedback.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39many(torch\u001b[39m.\u001b[39misnan(MOD\u001b[39m.\u001b[39mx[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])):\n",
            "\u001b[1;32m/its/home/jb739/esn2sparse/esn_feedback.ipynb Cell 19\u001b[0m in \u001b[0;36mMLPmetric.response\u001b[0;34m(self, Input, Y, it, backwardFlag)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn2sparse/esn_feedback.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=188'>189</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mReset(N_samples \u001b[39m*\u001b[39m \u001b[39m3\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn2sparse/esn_feedback.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=189'>190</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(T):\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn2sparse/esn_feedback.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=190'>191</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mESN_1step(Input[:, :, t], t)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn2sparse/esn_feedback.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=191'>192</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mForward()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn2sparse/esn_feedback.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=193'>194</a>\u001b[0m \u001b[39m# Compile responses from layer used to compute loss\u001b[39;00m\n",
            "\u001b[1;32m/its/home/jb739/esn2sparse/esn_feedback.ipynb Cell 19\u001b[0m in \u001b[0;36mMLPmetric.ESN_1step\u001b[0;34m(self, s, t)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn2sparse/esn_feedback.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=87'>88</a>\u001b[0m \u001b[39mif\u001b[39;00m t\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn2sparse/esn_feedback.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=88'>89</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mfbLayer\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn2sparse/esn_feedback.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=89'>90</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclone((\u001b[39m1\u001b[39m\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha)\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx[\u001b[39m0\u001b[39m]\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha \u001b[39m*\u001b[39m \\\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn2sparse/esn_feedback.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=90'>91</a>\u001b[0m             torch\u001b[39m.\u001b[39mtanh(torch\u001b[39m.\u001b[39;49mmatmul(s, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW_in)\u001b[39m+\u001b[39mtorch\u001b[39m.\u001b[39mmatmul(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW)\u001b[39m+\u001b[39mtorch\u001b[39m.\u001b[39mmatmul(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfbLayer], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_fb)))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn2sparse/esn_feedback.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=91'>92</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn2sparse/esn_feedback.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=92'>93</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclone((\u001b[39m1\u001b[39m\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha)\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx[\u001b[39m0\u001b[39m]\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha \u001b[39m*\u001b[39m \\\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcortex.inf.susx.ac.uk/its/home/jb739/esn2sparse/esn_feedback.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=93'>94</a>\u001b[0m             torch\u001b[39m.\u001b[39mtanh(torch\u001b[39m.\u001b[39mmatmul(s, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_in)\u001b[39m+\u001b[39mtorch\u001b[39m.\u001b[39mmatmul(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW)))\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ### Define parameters\n",
        "# swLR = np.logspace(-5,-7/3,9) # Sweep over these learning rates\n",
        "# nSeeds = 5 # No. runs per hyperparameter\n",
        "# seeds = list(sp.primerange(11111,33333))[0:nSeeds] # RNG seeds\n",
        "# complete = torch.zeros((len(swLR), len(seeds)))\n",
        "# for j, lr in enumerate(swLR):\n",
        "#     # if j!=0:\n",
        "#     #     continue\n",
        "#     for k, sd in enumerate(seeds):\n",
        "#         t = time.time()\n",
        "#         # Init RNG\n",
        "#         torch.manual_seed(sd)\n",
        "#         np.random.default_rng(sd)\n",
        "#         # Update hyperparameter\n",
        "#         importlib.reload(par)\n",
        "#         par.eta = swLR[j]\n",
        "#         par.fbLayer = 3\n",
        "#         par.saveFlag_FBWeights = False if not par.fbLayer else True \n",
        "#         expName = 'met_FB3_swLR_'+str(j)\n",
        "#         complete[j, k] = metric_esn_fb(expName, sd)\n",
        "#         print(f'j={j}/{len(swLR)};     k={k}/{len(seeds)}')\n",
        "#         print(f'*************************Run time: {time.time()-t}')\n",
        "# completeName = directory+'/data/met_FB3_swLR_0/complete.pt'\n",
        "# torch.save(complete, completeName)\n",
        "\n",
        "# print('*******************************FB1********************************')\n",
        "\n",
        "# swLR = np.logspace(-5,-7/3,9) # Sweep over these learning rates\n",
        "# nSeeds = 5 # No. runs per hyperparameter\n",
        "# seeds = list(sp.primerange(11111,33333))[0:nSeeds] # RNG seeds\n",
        "# complete = torch.zeros((len(swLR), len(seeds)))\n",
        "# for j, lr in enumerate(swLR):\n",
        "#     for k, sd in enumerate(seeds):\n",
        "#         t = time.time()\n",
        "#         # Init RNG\n",
        "#         torch.manual_seed(sd)\n",
        "#         np.random.default_rng(sd)\n",
        "#         # Update hyperparameter\n",
        "#         importlib.reload(par)\n",
        "#         par.eta = swLR[j]\n",
        "#         par.fbLayer = 2\n",
        "#         par.saveFlag_FBWeights = False if not par.fbLayer else True \n",
        "#         expName = 'met_FB2_swLR_'+str(j)\n",
        "#         complete[j, k] = metric_esn_fb(expName, sd)\n",
        "#         print(f'j={j}/{len(swLR)};     k={k}/{len(seeds)}')\n",
        "#         print(f'*************************Run time: {time.time()-t}')\n",
        "# completeName = directory+'/data/met_FB2_swLR_0/complete.pt'\n",
        "# torch.save(complete, completeName)\n",
        "\n",
        "# print('*******************************FB0********************************')\n",
        "\n",
        "swLR = np.logspace(-5,-7/3,9) # Sweep over these learning rates\n",
        "nSeeds = 5 # No. runs per hyperparameter\n",
        "seeds = list(sp.primerange(11111,33333))[0:nSeeds] # RNG seeds\n",
        "complete = torch.zeros((len(swLR), len(seeds)))\n",
        "for j, lr in enumerate(swLR):\n",
        "    for k, sd in enumerate(seeds):\n",
        "        t = time.time()\n",
        "        # Init RNG\n",
        "        torch.manual_seed(sd)\n",
        "        np.random.default_rng(sd)\n",
        "        # Update hyperparameter\n",
        "        importlib.reload(par)\n",
        "        par.eta = swLR[j]\n",
        "        par.fbLayer = 1\n",
        "        par.saveFlag_FBWeights = False if not par.fbLayer else True \n",
        "        expName = 'met_FB1_swLR_'+str(j)\n",
        "        complete[j, k] = metric_esn_fb(expName, sd)\n",
        "        print(f'j={j}/{len(swLR)};     k={k}/{len(seeds)}')\n",
        "        print(f'*************************Run time: {time.time()-t}')\n",
        "completeName = directory+'/data/met_FB1_swLR_0/complete.pt'\n",
        "torch.save(complete, completeName)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "0face943",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6.675, 2.875, 0.4307116104868914\n"
          ]
        }
      ],
      "source": [
        "c = 534\n",
        "g = 230\n",
        "print(f'{c*5*9/60/60}, {g*5*9/60/60}, {g/c}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "80762c57",
      "metadata": {},
      "source": [
        "# Run parameter sweep (MetricLearning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30567a8c",
      "metadata": {},
      "outputs": [],
      "source": [
        "complete=np.ones((9,5))\n",
        "complete[-1,0:2]=-1\n",
        "completeName = directory+'/data/met_FB3_swLR_0/complete.pt'\n",
        "torch.save(complete, completeName)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "19b65aae",
      "metadata": {
        "id": "19b65aae"
      },
      "source": [
        "# Plot data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "600e634a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "600e634a",
        "outputId": "9cd64612-b412-4422-f3f5-f179e36a7907"
      },
      "outputs": [],
      "source": [
        "# ### Load Triplet data\n",
        "# inputDir = directory+'/data/'+experiment    # Storage directory for input/label data\n",
        "# met = torch.load(inputDir + 'met_save.pt')\n",
        "# l_tri = torch.load(inputDir + 'loss_triplet.pt')\n",
        "# l_out = torch.load(inputDir + 'loss_out.pt')\n",
        "# a_tri = torch.load(inputDir + 'acc_triplet.pt')\n",
        "# a_out = torch.load(inputDir + 'acc_out.pt')\n",
        "# dist = torch.load(inputDir + 'dist_triplet.pt')\n",
        "# nt_dist = dist.shape[0] \n",
        "# nt_out = l_out.shape[0]\n",
        "# nt_met = met.shape[0]\n",
        "# N_triplet=45000\n",
        "# N_out=5000\n",
        "# batch_size=64\n",
        "# eta_t=0.0002\n",
        "# eta_o = 0.001\n",
        "# eta_t_tau = 40000.0\n",
        "# eta_o_tau = 4000.0\n",
        "# N_class=10\n",
        "# margin=2\n",
        "# save_N = 100 #100 # # of saved epochs\n",
        "# save_every = np.floor(N_triplet / save_N) # Save data every <> epohsChoice 3\n",
        "# save_Nsamples = 100 # # of inputs from each class for which to save resonses\n",
        "\n",
        "### Load Classic data\n",
        "experiment = 'met_FB0_swLR_0'\n",
        "expDir = directory+'/data/'+experiment # To read in saved data\n",
        "sd_name='11113'\n",
        "inputDir = expDir#+'/'+sd_name  \n",
        "figDir = directory+'/figs/met/'+experiment # To export figures\n",
        "print(figDir)\n",
        "if not os.path.exists(figDir):\n",
        "    os.mkdir(figDir)\n",
        "\n",
        "RESP = torch.load(inputDir + '/respSave'+sd_name+'.pt')\n",
        "print(f'Num layers = {len(RESP)}')\n",
        "lossTr = torch.load(inputDir + '/lossTr'+sd_name+'.pt')\n",
        "accTr = torch.load(inputDir + '/accTr'+sd_name+'.pt')\n",
        "accVal = torch.load(inputDir + '/accVal'+sd_name+'.pt')\n",
        "weights = torch.load(inputDir + '/weightSave'+sd_name+'.pt')\n",
        "nt = RESP[0].shape[-1]\n",
        "kernel = np.ones(50)/50\n",
        "\n",
        "### Set figure properties\n",
        "import matplotlib\n",
        "matplotlib.rcParams['savefig.dpi'] = 300\n",
        "matplotlib.rcParams.update({'font.size': 6})\n",
        "matplotlib.rcParams['svg.fonttype'] = 'none'\n",
        "matplotlib.rcParams['savefig.format'] = 'svg'\n",
        "matplotlib.rcParams['font.family'] = 'sans-serif'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91085166",
      "metadata": {},
      "outputs": [],
      "source": [
        "weights.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e873e09",
      "metadata": {},
      "outputs": [],
      "source": [
        "# saveflag = True\n",
        "saveflag = False\n",
        "\n",
        "# fig = pl.figure(figsize=tuple(np.array((6.,4.))/2.54)); ax = pl.axes()\n",
        "# ax.spines[['top','right']].set_visible(False)\n",
        "# layer = 2\n",
        "# c=0\n",
        "# times=[0, 4]\n",
        "\n",
        "# ### Plot responses\n",
        "# # t=0\n",
        "# # for i in range(c*par.nClass,(c+1)*par.nClass):\n",
        "# #             pl.plot(RESP[layer][t,i,::10,:].transpose(), linewidth=0.5)\n",
        "# for c in [0, 1, 9]:\n",
        "#     for j, t in enumerate(times):\n",
        "#         for i in range(c*par.nClass,(c+1)*par.nClass):\n",
        "#             pl.plot(RESP[layer][t,i,:,:].transpose(), linewidth=0.5)\n",
        "#         if j==0:\n",
        "#             ax.xaxis.set_ticks((0,nt)); ax.yaxis.set_ticks((0,0.2))\n",
        "#         elif j==1:\n",
        "#             ax.xaxis.set_ticks((0,nt)); ax.yaxis.set_ticks((0,10,20))\n",
        "#         if saveflag:\n",
        "#             pl.savefig(figDir+'/resp_c'+str(c)+'t'+str(j)+'.svg', format=\"svg\")\n",
        "#         ax.cla() \n",
        "\n",
        "# ### Plot accuracy\n",
        "# fig = pl.figure(figsize=tuple(np.array((6.,4.))/2.54)); ax = pl.axes()\n",
        "# ax.spines[['top','right']].set_visible(False)\n",
        "# pl.plot(np.linspace(1,par.nEpochs,par.nEpochs),np.convolve(np.pad(accTr, (50,50), mode='edge'),kernel,mode='same')[50:-50], linewidth=1.0)\n",
        "# pl.plot(np.linspace(1,par.nEpochs,par.nEpochs),np.convolve(np.pad(accVal, (50,50), mode='edge'),kernel,mode='same')[50:-50], linewidth=1.0)\n",
        "# # pl.plot(accTr, linewidth=0.5); pl.plot(accVal, linewidth=0.5)\n",
        "# ax.xaxis.set_ticks((0,par.nEpochs)); ax.yaxis.set_ticks((0,1))\n",
        "# if saveflag:\n",
        "#     pl.savefig(figDir+'/accTrVal.svg', format=\"svg\")\n",
        "\n",
        "### Plot Feedback Weight Evolution\n",
        "if par.saveFlag_FBWeights:\n",
        "    fig = pl.figure(figsize=tuple(np.array((20.,5.))/2.54)); ax = pl.axes()\n",
        "    ax.spines[['top','right']].set_visible(False)\n",
        "    for j in range(0,1000,100):\n",
        "        pl.plot(np.linspace(1,par.nEpochs,par.nSave+1),weights[::2,j,:].transpose(), linewidth=0.5)\n",
        "ax.xaxis.set_ticks((0,par.nEpochs)); ax.yaxis.set_ticks((-.05,0,.05))\n",
        "if saveflag:\n",
        "    pl.savefig(figDir+'/fbWeights.svg', format=\"svg\")\n",
        "\n",
        "# ### Correlation between met responses\n",
        "# layer = 1\n",
        "# fig = pl.figure(figsize=tuple(np.array((4.,4.))/2.54)); ax = pl.axes()\n",
        "# # imdata = ax.imshow(stats.zscore(np.reshape(RESP[1][0,:,:,:],[RESP[1].shape[1], RESP[1].shape[2]*RESP[1].shape[3]]), 1).transpose(),vmin=-1.0, vmax=1.0)\n",
        "# c0 = np.matmul(stats.zscore(np.reshape(RESP[layer][0,:,:,-1],[RESP[layer].shape[1], RESP[layer].shape[2]]), 1), stats.zscore(np.reshape(RESP[layer][0,:,:,-1],[RESP[layer].shape[1], RESP[layer].shape[2]]), 1).transpose()) / (RESP[layer].shape[2])\n",
        "# imdata = ax.imshow(c0,vmin=-1.0, vmax=1.0)\n",
        "# cb = fig.colorbar(imdata, ticks=[-1.0, 0.0, 1.0])\n",
        "# if saveflag:\n",
        "#     pl.savefig(directory+'/figs/'+prefix+'met0_corr.svg', format=\"svg\")\n",
        "# fig = pl.figure(figsize=tuple(np.array((4.,4.))/2.54)); ax = pl.axes()\n",
        "# c1 = np.matmul(stats.zscore(np.reshape(RESP[layer][19,:,:,-1],[RESP[layer].shape[1], RESP[layer].shape[2]]), 1), stats.zscore(np.reshape(RESP[layer][19,:,:,-1],[RESP[layer].shape[1], RESP[layer].shape[2]]), 1).transpose()) / (RESP[layer].shape[2])\n",
        "# imdata = ax.imshow(c1,vmin=-1.0, vmax=1.0)\n",
        "# cb = fig.colorbar(imdata, ticks=[-1.0, 0.0, 1.0])\n",
        "# if saveflag:\n",
        "#     pl.savefig(directory+'/figs/'+prefix+'met1_corr.svg', format=\"svg\") \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "82efca1a",
      "metadata": {
        "id": "82efca1a"
      },
      "source": [
        "### Plot triplet distances and loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e90dcb30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "e90dcb30",
        "outputId": "2637691c-fdaa-4170-c851-b3d7de14cff7"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "from matplotlib import pyplot as pl\n",
        "# saveflag = True\n",
        "saveflag = False\n",
        "tripletflag = True\n",
        "# tripletflag = False\n",
        "\n",
        "kernel = np.ones(50)/50\n",
        "prefix = experiment#'classic_original_'\n",
        "matplotlib.rcParams['savefig.dpi'] = 300\n",
        "matplotlib.rcParams['axes.labelsize'] = 6\n",
        "matplotlib.rcParams['svg.fonttype'] = 'none'\n",
        "matplotlib.rcParams['savefig.format'] = 'svg'\n",
        "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
        "\n",
        "###Plot distances\n",
        "if tripletflag:\n",
        "  fig = pl.figure(figsize=tuple(np.array((6.,4.))/2.54)); ax = pl.axes()\n",
        "  ax.spines[['top','right']].set_visible(False)\n",
        "  # pl.plot(np.log10(np.linspace(1,nt_dist,nt_dist)),dist[:,0]); pl.plot(np.log10(np.linspace(1,nt_dist,nt_dist)),dist[:,1], linestyle='dashed')\n",
        "  pl.plot(np.linspace(1,nt_dist,nt_dist),dist[:,0], linewidth=0.5)\n",
        "  pl.plot(np.linspace(1,nt_dist,nt_dist),dist[:,1], linewidth=0.5)\n",
        "  ax.set_xlim(xmin=0, xmax=np.log10(nt_dist)); ax.set_ylim(ymin=0.0, ymax=20)\n",
        "  # ax.xaxis.set_ticks((np.log10((1,10,100,1000,10000)))); ax.yaxis.set_ticks((0,15,30))\n",
        "  ax.xaxis.set_ticks((0,nt_dist)); ax.yaxis.set_ticks((0,20))\n",
        "  # ax.tick_params(axis='both', which='major', labelsize=6.0)\n",
        "  if saveflag:\n",
        "      pl.savefig(directory+'/figs/'+prefix+'distances.svg', format=\"svg\")\n",
        "\n",
        "###Plot triplett loss\n",
        "if tripletflag:\n",
        "  fig = pl.figure(figsize=tuple(np.array((6.,4.))/2.54)); ax = pl.axes()\n",
        "  ax.spines[['top','right']].set_visible(False)\n",
        "  # pl.plot(np.log10(np.linspace(1,nt_dist,nt_dist)),l_tri); pl.plot(np.log10(np.linspace(1,nt_dist,nt_distX_tr.shape)),np.convolve(l_tri,kernel,mode='same')); \n",
        "  pl.plot(np.linspace(1,nt_dist,nt_dist),l_tri, linewidth=0.5)\n",
        "  pl.plot(np.linspace(1,nt_dist,nt_dist),np.convolve(np.pad(l_tri, (50,50), mode='edge'),kernel,mode='same')[50:-50], linewidth=1.0); \n",
        "  # ax.set_xlim(xmin=0, xmax=np.log10(nt_dist)); ax.set_ylim(ymin=0.0, ymax=0.5*1.05*np.max(l_tri))\n",
        "  ax.set_xlim(xmin=0, xmax=np.log10(nt_dist)); ax.set_ylim(ymin=0.0, ymax=1.0)\n",
        "  # ax.xaxis.set_ticks((np.log10((1,10,100,1000,10000)))); ax.yaxis.set_ticks((0,5))\n",
        "  ax.xaxis.set_ticks((0,nt_dist)); ax.yaxis.set_ticks((0,1))\n",
        "  ax.tick_params(axis='both', which='major', labelsize=6.0)\n",
        "  if saveflag:\n",
        "      pl.savefig(directory+'/figs/'+prefix+'loss_triplet.svg', format=\"svg\")\n",
        "\n",
        "###Plot triplet accuracy\n",
        "if tripletflag:\n",
        "  fig = pl.figure(figsize=tuple(np.array((6.,4.))/2.54)); ax = pl.axes()\n",
        "  ax.spines[['top','right']].set_visible(False)\n",
        "  pl.plot(np.linspace(1,nt_dist,nt_dist),a_tri*100, linewidth=0.5)\n",
        "  pl.plot(np.linspace(1,nt_dist,nt_dist),np.convolve(np.pad(a_tri*100, (50,50), mode='edge'),kernel,mode='same')[50:-50], linewidth=1.0); \n",
        "  ax.set_xlim(xmin=0, xmax=np.log10(nt_dist)); ax.set_ylim(ymin=0.0, ymax=100.)\n",
        "  # ax.xaxis.set_ticks((np.log10((1,10,100,1000,10000)))); ax.yaxis.set_ticks((0,5))\n",
        "  ax.xaxis.set_ticks((0,nt_dist)); ax.yaxis.set_ticks((0,100))\n",
        "  ax.tick_params(axis='both', which='major', labelsize=6.0)\n",
        "  if saveflag:\n",
        "      pl.savefig(directory+'/figs/'+prefix+'acc_triplet.svg', format=\"svg\")\n",
        "\n",
        "###Plot output loss\n",
        "fig = pl.figure(figsize=tuple(np.array((6.,4.))/2.54)); ax = pl.axes()\n",
        "ax.spines[['top','right']].set_visible(False)\n",
        "# pl.plot(np.log10(np.linspace(1,nt_dist,nt_dist)),l_tri); pl.plot(np.log10(np.linspace(1,nt_dist,nt_dist)),np.convolve(l_tri,kernel,mode='same')); \n",
        "pl.plot(np.linspace(1,nt_out,nt_out),l_out, linewidth=0.5)\n",
        "pl.plot(np.linspace(1,nt_out,nt_out),np.convolve(np.pad(l_out, (50,50), mode='edge'),kernel,mode='same')[50:-50], linewidth=1.0); \n",
        "ax.set_xlim(xmin=0, xmax=np.log10(nt_out)); ax.set_ylim(ymin=0.0, ymax=0.8)\n",
        "# ax.xaxis.set_ticks((np.log10((1,10,100,1000,10000)))); ax.yaxis.set_ticks((0,5))\n",
        "ax.xaxis.set_ticks((0,nt_out)); ax.yaxis.set_ticks((0.0,0.8))\n",
        "ax.tick_params(axis='both', which='major', labelsize=6.0)\n",
        "ax.set_xticklabels((0, nt_out), fontdict={'family': 'sans-serif', 'size':5})\n",
        "ax.set_yticklabels((0.5,0.8), fontdict={'family': 'sans-serif', 'size':5})\n",
        "if saveflag:\n",
        "    pl.savefig(directory+'/figs/'+prefix+'loss_out.svg', format=\"svg\")\n",
        "\n",
        "###Plot output accuracy\n",
        "fig = pl.figure(figsize=tuple(np.array((6.,4.))/2.54)); ax = pl.axes()\n",
        "ax.spines[['top','right']].set_visible(False)\n",
        "pl.plot(np.linspace(1,nt_out,nt_out),a_out*100, linewidth=0.5)\n",
        "pl.plot(np.linspace(1,nt_out,nt_out),np.convolve(np.pad(a_out*100, (50,50), mode='edge'),kernel,mode='same')[50:-50], linewidth=1.0); \n",
        "ax.set_xlim(xmin=0, xmax=np.log10(nt_out)); ax.set_ylim(ymin=0.0, ymax=100.)\n",
        "# ax.xaxis.set_ticks((np.log10((1,10,100,1000,10000)))); ax.yaxis.set_ticks((0,5))\n",
        "ax.xaxis.set_ticks((0,nt_out)); ax.yaxis.set_ticks((0,100))\n",
        "ax.tick_params(axis='both', which='major', labelsize=6.0)\n",
        "if saveflag:\n",
        "    pl.savefig(directory+'/figs/'+prefix+'acc_out.svg', format=\"svg\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3a8b3d98",
      "metadata": {
        "id": "3a8b3d98"
      },
      "source": [
        "### Plot met responses over training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c442906",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        },
        "id": "1c442906",
        "outputId": "97f4589d-d70e-4cb2-e668-7042ecb759a8"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as pl\n",
        "from scipy import stats\n",
        "pl.rcParams['savefig.dpi'] = 400\n",
        "\n",
        "saveflag = True\n",
        "# saveflag = False\n",
        "\n",
        "### Raw met reponses\n",
        "fig = pl.figure(figsize=tuple(np.array((6.,5.))/2.54)); ax = pl.axes()\n",
        "ax.spines[['top','right']].set_visible(False)\n",
        "plmet = np.squeeze(met[0,:,199])\n",
        "pl.plot(np.linspace(1,nt_met,nt_met)*save_every,plmet)\n",
        "ax.set_xlim(xmin=0, xmax=nt_met*save_every); ax.set_ylim(ymin=0.0, ymax=1.05*np.max(plmet))\n",
        "ax.xaxis.set_ticks((0,nt_met*save_every)); ax.yaxis.set_ticks((0,))\n",
        "pl.show()\n",
        "\n",
        "fig = pl.figure(figsize=tuple(np.array((20.,5.))/2.54)); ax = pl.axes()\n",
        "c0 = stats.zscore(np.squeeze(met[0,:,:]),0)\n",
        "imdata = ax.imshow(c0,vmin=0.0, vmax=2.0)\n",
        "cb = fig.colorbar(imdata, ticks=[0., 2.0])\n",
        "if saveflag:\n",
        "    pl.savefig(directory+'/figs/'+prefix+'met0.svg', format=\"svg\")\n",
        "fig = pl.figure(figsize=tuple(np.array((20.,5.))/2.54)); ax = pl.axes()\n",
        "c0 = stats.zscore(np.squeeze(met[-1,:,:]),0)\n",
        "imdata = ax.imshow(c0,vmin=0.0, vmax=2.0)\n",
        "cb = fig.colorbar(imdata, ticks=[0., 2.0])\n",
        "if saveflag:\n",
        "    pl.savefig(directory+'/figs/'+prefix+'met1.svg', format=\"svg\")\n",
        "\n",
        "### Correlation between met responses\n",
        "fig = pl.figure(figsize=tuple(np.array((5.,5.))/2.54)); ax = pl.axes()\n",
        "c0 = np.matmul(stats.zscore(np.squeeze(met[0,:,:]),0).transpose(), stats.zscore(np.squeeze(met[0,:,:]),0)) / met.shape[1]\n",
        "imdata = ax.imshow(c0,vmin=-1.0, vmax=1.0)\n",
        "cb = fig.colorbar(imdata, ticks=[-1.0, 0.0, 1.0])\n",
        "if saveflag:\n",
        "    pl.savefig(directory+'/figs/'+prefix+'met0_corr.svg', format=\"svg\")\n",
        "fig = pl.figure(figsize=tuple(np.array((5.,5.))/2.54)); ax = pl.axes()\n",
        "c1 = np.matmul(stats.zscore(np.squeeze(met[-1,:,:]),0).transpose(), stats.zscore(np.squeeze(met[-1,:,:]),0)) / met.shape[1]\n",
        "imdata = ax.imshow(c1,vmin=-1.0, vmax=1.0)\n",
        "cb = fig.colorbar(imdata, ticks=[-1.0, 0.0, 1.0])\n",
        "if saveflag:\n",
        "    pl.savefig(directory+'/figs/'+prefix+'met1_corr.svg', format=\"svg\") \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9f4c2a58",
      "metadata": {
        "id": "9f4c2a58"
      },
      "source": [
        "# Plot weights"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:venv] *",
      "language": "python",
      "name": "conda-env-venv-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "6ca3cabd64c821540d03c27655fd3203d564282a95e64fe84544566440549db9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
